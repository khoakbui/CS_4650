
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍
#################################################
# file to edit: CS4650_hw2_release_fall2025.ipynb͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍

import os

# Importing required libraries
# Do not change the libraries already imported or import additional libraries
import torch
import torch.nn as nn
import random
import numpy as np
from collections import Counter
import re
import html
import pandas as pd
from tqdm import tqdm
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, RandomSampler, DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# SOME UTILITY FUNCTIONS - DO NOT CHANGE
def save_checkpoint(model, model_name, loss_fn='ce'):
    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')
    os.makedirs(os.path.join(os.getcwd(), 'model_weights'), exist_ok=True)
    checkpoint = { # create a dictionary with all the state information
        'model_state_dict': model.state_dict()
    }
    torch.save(checkpoint, file_path)
    print(f"Checkpoint saved to {file_path}")

def load_checkpoint(model, model_name, loss_fn='ce', map_location='cpu'):
    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')
    checkpoint = torch.load(file_path, map_location=map_location) # load the checkpoint, ensure correct device
    model.load_state_dict(checkpoint['model_state_dict'])

# Defining global constants - DO NOT CHANGE THESE VALUES (except batch size if you have memory issues)
RANDOM_SEED = 42
PADDING_VALUE = 0
UNK_VALUE     = 1
BATCH_SIZE = 128

torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
device = torch.device('cuda' if torch.cuda.is_available() else('mps' if torch.backends.mps.is_available() else 'cpu'))

# example code taken from fast-bert
# DO NOT CHANGE THIS CELL

def spec_add_spaces(t: str) -> str:
    "Add spaces around / and # in `t`. \n"
    return re.sub(r"([/#\n])", r" \1 ", t)

def rm_useless_spaces(t: str) -> str:
    "Remove multiple spaces in `t`."
    return re.sub(" {2,}", " ", t)

def replace_multi_newline(t: str) -> str:
    return re.sub(r"(\n(\s)*){2,}", "\n", t)

def fix_html(x: str) -> str:
    "List of replacements from html strings in `x`."
    re1 = re.compile(r"  +")
    x = (
        x.replace("#39;", "'")
        .replace("amp;", "&")
        .replace("#146;", "'")
        .replace("nbsp;", " ")
        .replace("#36;", "$")
        .replace("\\n", "\n")
        .replace("quot;", "'")
        .replace("<br />", "\n")
        .replace('\\"', '"')
        .replace(" @.@ ", ".")
        .replace(" @-@ ", "-")
        .replace(" @,@ ", ",")
        .replace("\\", " \\ ")
    )
    return re1.sub(" ", html.unescape(x))

def clean_text(input_text):
    text = fix_html(input_text)
    text = replace_multi_newline(text)
    text = spec_add_spaces(text)
    text = rm_useless_spaces(text)
    text = text.strip()
    return text


def generate_vocab_map(df, cutoff=2):
    """
    This method takes a dataframe and builds a vocabulary to unique number map.
    It uses the cutoff argument to remove rare words occurring <= cutoff times.
    "" and "UNK" are reserved tokens in our vocab that will be useful later.
    You'll also find the Counter imported for you to be useful as well.

    Args:
        df (pandas.DataFrame) : The entire dataset this mapping is built from
        cutoff (int) : We exclude words from the vocab that appear less than or equal to cutoff

    Returns:
        vocab (dict[str] = int) : In vocab, each str is a unique token, and each dict[str] is a
            unique integer ID. Only elements that appear > cutoff times appear in vocab.
        reversed_vocab (dict[int] = str) : A reversed version of vocab, which allows us to retrieve
            words given their unique integer ID. This map will allow us to "decode" integer
            sequences we'll encode using vocab!
    """
    vocab          = {"": PADDING_VALUE, "UNK": UNK_VALUE}
    reversed_vocab = None

    ## YOUR CODE STARTS HERE ##
    # hint: start by iterating over df["tokenized"]
    # Flatten all tokenized lists into one big list of words
    all_tokens = [token for tokens in df["tokenized"] for token in tokens]

    # Count frequency of each word
    counter = Counter(all_tokens)

    # Start indexing after the reserved tokens
    index = len(vocab)

    # Add words that appear more than cutoff times
    for word, count in counter.items():
        if count > cutoff:
            vocab[word] = index
            index += 1

    # Create reversed mapping (index → word)
    reversed_vocab = {idx: word for word, idx in vocab.items()}
    ## YOUR CODE ENDS HERE ##

    return vocab, reversed_vocab


class HeadlineDataset(Dataset):
    """
    This class takes a Pandas DataFrame and wraps in a Torch Dataset.
    Read more about Torch Datasets here:
    https://pytorch.org/tutorials/beginner/basics/data_tutorial.html
    """
    def __init__(self, vocab, df, max_length=200):
        """
        Initialize the class with appropriate instance variables. In this method, we
        STRONGLY recommend storing the dataframe itself as an instance variable, and
        keeping this method very simple. Leave processing to __getitem__.

        Args:
            vocab (dict[str] = int) : In vocab, each str is a unique token, and each dict[str] is a
                unique integer ID. Only elements that appear > cutoff times appear in vocab.
            df (pandas.DataFrame) : The entire dataset this mapping is built from
            max_length (int) : The max length of a headline we'll allow in our dataset.

        Returns:
            None
        """

        ## YOUR CODE STARTS HERE - initialize parameters ##
        # keep it simple; store refs and use them in __getitem__
        self.vocab = vocab
        self.df = df
        self.max_length = max_length
        ## YOUR CODE ENDS HERE ##

    def __len__(self):
        """
        This method returns the length of the underlying dataframe,
        Args:
            None
        Returns:
            df_len (int) : The length of the underlying dataframe
        """

        df_len = None

        ## YOUR CODE STARTS HERE ##
        return len(self.df)
        ## YOUR CODE ENDS HERE ##

        return df_len

    def __getitem__(self, index: int):
        """
        This method converts a dataframe row (row["tokenized"]) to an encoded torch LongTensor,
        using our vocab map created using generate_vocab_map. Restricts the encoded headline
        length to max_length.

        The purpose of this method is to convert the row - a list of words - into a corresponding
        list of numbers.

        i.e. using a map of {"hi": 2, "hello": 3, "UNK": 0}
        this list ["hi", "hello", "NOT_IN_DICT"] will turn into [2, 3, 0]

        Args:
            index (int) : The index of the dataframe we want to retrieve.

        Returns:
            tokenized_word_tensor (torch.LongTensor) : A 1D tensor of type Long, that has each
                token in the dataframe mapped to a number. These numbers are retrieved from the
                vocab_map we created in generate_vocab_map.

                IMPORTANT: If we filtered out the word because it's infrequent (and it doesn't
                exist in the vocab) we need to replace it w/ the UNK token

            curr_label (int) : Label index of the class between 0 to len(num_classes) - 1 representing which
            class label does this data instance belong to
        """

        tokenized_word_tensor = None
        curr_label            = None

        ## YOUR CODE STARTS HERE ##
        row = self.df.iloc[index]

        # tokens and label
        tokens = row["tokenized"]
        curr_label = int(row["target"])

        # map tokens -> ids, use UNK for OOV
        ids = [self.vocab.get(tok, UNK_VALUE) for tok in tokens]

        # truncate to max_length
        if self.max_length is not None:
            ids = ids[: self.max_length]

        # avoid empty sequences (rare) by inserting a single PAD
        if len(ids) == 0:
            ids = [PADDING_VALUE]

        tokenized_word_tensor = torch.tensor(ids, dtype=torch.long)

        return tokenized_word_tensor, curr_label
        ## YOUR CODE ENDS HERE ##

        return tokenized_word_tensor, curr_label


def collate_fn(batch, padding_value=PADDING_VALUE):
    """
    This function is passed as a parameter to Torch DataSampler. collate_fn collects
    batched rows, in the form of tuples, from a DataLoader and applies some final
    pre-processing.

    Objective:
    In our case, we need to take the batched input array of 1D tokenized_word_tensors,
    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors
    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.

    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)

    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html

    :param batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)] of length BATCH_SIZE
    :param padding_value: int

    :return padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))
    :return y_labels: 1D FloatTensor of shape (BATCH_SIZE)
    """

    padded_tokens, y_labels = None, None

    ## YOUR CODE STARTS HERE - take the input and target from batch, pad the tokens, convert batches to tensor ##
    # unpack
    token_tensors, labels = zip(*batch)         # lists/tuples of len B

    # pad to the longest sequence in this batch
    padded_tokens = pad_sequence(
        token_tensors,
        batch_first=True,
        padding_value=padding_value
    )                                           # shape [B, T], dtype long

    # labels as a tensor
    # CrossEntropyLoss expects long class indices; if your code later wants float, use .float()
    y_labels = torch.tensor(labels, dtype=torch.long)

    return padded_tokens, y_labels
    ## YOUR CODE ENDS HERE ##

    return padded_tokens, y_labels


class NBOW(nn.Module):
    # Instantiate layers for your model-
    #
    # Your model architecture will be a feed-forward neural network.
    #
    # You'll need 2 nn.Modules:
    # 1. An embeddings layer (see nn.Embedding)
    # 2. A linear layer (see nn.Linear)
    #
    # HINT: In the forward step, the BATCH_SIZE is the first dimension.
    #
    def __init__(self, vocab_size, embedding_dim, num_classes=20):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PADDING_VALUE)
        # remove bias so the test matches expected logits
        self.fc = nn.Linear(embedding_dim, num_classes, bias=False)
        ## YOUR CODE ENDS HERE ##

    # Complete the forward pass of the model.
    #
    # Use the output of the embedding layer to create
    # the average vector, which will be input into the
    # linear layer.
    #
    # args:
    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))
    #     This is the same output that comes out of the collate_fn function you completed
    def forward(self, x):
        ## Hint: Make sure to handle the case where x contains pad tokens. We don't want to consider them in our average.
        ## YOUR CODE STARTS HERE ##
        emb = self.embedding(x)                                 # (B, T, D)
        mask = (x != PADDING_VALUE).unsqueeze(-1).float()       # (B, T, 1)
        emb_sum = (emb * mask).sum(dim=1)                       # (B, D)
        lengths = mask.sum(dim=1).clamp(min=1.0)                # (B, 1)
        h_avg = emb_sum / lengths                               # (B, D)
        return self.fc(h_avg)
        ## YOUR CODE ENDS HERE ##

    def get_embeddings(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        return self.embedding(x)
        ### YOUR CODE ENDS HERE ###

    def set_embedding_weight(self, weight):
        '''
        This function sets the embedding weights to the input weight. Ensure you aren't recording gradients for this.
        Hint: Refer to nn.Parameter to do this.
        Args:
            weight: torch.tensor of shape (vocab_size, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.embedding.weight.data.copy_(weight)
        ### YOUR CODE ENDS HERE ###
    def get_h_avg(self, x):
        '''
        This function returns the average of the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        emb = self.embedding(x)
        mask = (x != PADDING_VALUE).unsqueeze(-1).float()
        emb_sum = (emb * mask).sum(dim=1)
        lengths = mask.sum(dim=1).clamp(min=1.0)
        return emb_sum / lengths
        ### YOUR CODE ENDS HERE ###

# DO NOT CHANGE THIS CELL
def get_accuracy_and_f1_score(y_true, y_predicted):
    """
    This function takes in two numpy arrays and computes the accuracy and F1 score
    between them. You can use the imported sklearn functions to do this.

    Args:
        y_true (list) : A 1D numpy array of ground truth labels
        y_predicted (list) : A 1D numpy array of predicted labels

    Returns:
        accuracy (float) : The accuracy of the predictions
        f1_score (float) : The F1 score of the predictions
    """

    # Get the accuracy
    accuracy = accuracy_score(y_true, y_predicted)

    # Get the F1 score
    f1 = f1_score(y_true, y_predicted, average='macro')

    return accuracy, f1



def get_criterion(loss_type='ce'):
    criterion = None

    ## YOUR CODE STARTS HERE ##
    if loss_type == 'ce':
        criterion = nn.CrossEntropyLoss()
    else:
        raise ValueError(f"Unsupported loss type: {loss_type}")
    ## YOUR CODE ENDS HERE ##

    return criterion

def get_optimizer(model, learning_rate):
    """
    This function takes a model and a learning rate, and returns an optimizer.
    Feel free to experiment with different optimizers.
    """
    optimizer = None

    ## YOUR CODE STARTS HERE ##
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    ## YOUR CODE ENDS HERE ##

    return optimizer

def train_loop(model, criterion, optimizer, iterator, epoch, save_every=10):
    """
    This function is used to train a model for one epoch.
    :param model: The model to be trained
    :param criterion: The loss function
    :param optim: The optimizer
    :param iterator: The training data iterator
    :return: The average loss for this epoch
    """
    model.train() # Is used to put the model in training mode
    total_loss = 0
    for x, y in tqdm(iterator, total=len(iterator), desc="Training Model"):
        ### YOUR CODE STARTS HERE ###
        # Move batch to same device as model
        x = x.to(device)
        y = y.to(device)

        # ---- forward ----
        logits = model(x)
        loss = criterion(logits, y)

        # ---- backward + optimize ----
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        ### YOUR CODE ENDS HERE ###

    average_loss = total_loss / len(iterator)
    return average_loss

def val_loop(model, criterion, iterator):
    """
    This function is used to evaluate a model on the validation set.
    :param model: The model to be evaluated
    :param iterator: The validation data iterator
    :return: true: a Python boolean array of all the ground truth values
             pred: a Python boolean array of all model predictions.
            average_loss: The average loss over the validation set
    """

    true, pred = [], []
    total_loss = 0
    model.eval()
    for x, y in tqdm(iterator, total=len(iterator), desc="Evaluating Model"):
    ### YOUR CODE STARTS HERE ###
        # Move data to same device as model
        x = x.to(device)
        y = y.to(device)

        # Forward pass
        logits = model(x)
        loss = criterion(logits, y)
        total_loss += loss.item()

        # Predictions
        preds = torch.argmax(logits, dim=1)

        # Store for metric computation
        true.extend(y.cpu().numpy())
        pred.extend(preds.cpu().numpy())
    ### YOUR CODE ENDS HERE ###
    average_loss = total_loss / len(iterator)
    return true, pred, average_loss

# Assigning hyperparameters and training parameters
# Experiment with different values for these hyperparaters to optimize your model's performance
def get_hyperparams_nbow():
  ### your hyper parameters
  learning_rate = 2e-3     # float
  epochs        = 6        # int
  embedding_dim = 200      # int (try 100–300)
  ###
  return learning_rate, epochs, embedding_dim

def get_nbow_model(vocab_size, embedding_dim):
    """
    This function returns an instance of the NBOW model.
    """
    model = None
    # Define a model and return
    # YOUR CODE STARTS HERE
    model = NBOW(vocab_size=vocab_size, embedding_dim=embedding_dim, num_classes=len(id2label))
    model = model.to(device)  # move to CUDA/MPS/CPU as defined in setup
    # YOUR CODE ENDS HERE
    return model

class DAN(nn.Module):
    # Instantiate layers for your model-
    #
    # Your model architecture will be a feed-forward neural network.
    #
    # You'll need 4 nn.Modules:
    # 1. An embeddings layer (see nn.Embedding)
    # 2. A linear layer (see nn.Linear)
    # 3. A ReLU activation (see nn.ReLU)
    # 4. A linear layer (see nn.Linear)
    #
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes=20):
        # vocab_size is the size of the vocabulary
        # embedding_dim is the dimension of the word embeddings
        # hidden_dim is the dimension of the hidden layer outputs, i.e., the 2nd module as per the definition above
        super().__init__()
        ## YOUR CODE STARTS HERE ##
        # 1) NO padding_idx here (test expects non-zero PAD row to survive)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # 2) Hidden MLP: avg-emb -> ReLU -> logits
        self.proj1 = nn.Linear(embedding_dim, hidden_dim)
        self.relu  = nn.ReLU()
        self.out   = nn.Linear(hidden_dim, num_classes)
        ## YOUR CODE ENDS HERE ##

    # helper method
    def _masked_average(self, x: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:
        """
        x:  (B, T) Long
        emb:(B, T, D) Float
        returns h_avg: (B, D)
        """
        mask = (x != PADDING_VALUE).unsqueeze(-1)            # (B, T, 1) bool
        emb_masked = emb * mask                              # zero-out PAD positions
        lengths = mask.sum(dim=1).clamp(min=1).to(emb.dtype) # (B, 1)
        h_avg = emb_masked.sum(dim=1) / lengths              # (B, D)
        return h_avg

    # Complete the forward pass of the model.
    #
    # Use the output of the embedding layer to create
    # the average vector, which will be input into the
    # linear layer.
    #
    # args:
    # x - 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))
    #     This is the same output that comes out of the collate_fn function you completed
    def forward(self, x):
        ## YOUR CODE STARTS HERE ##
        # x: (B, T)
        emb  = self.embedding(x)                 # (B, T, D)  (PAD row stays as-is)
        havg = self._masked_average(x, emb)      # (B, D)     (PAD ignored here)
        h    = self.relu(self.proj1(havg))       # (B, H)
        logits = self.out(h)                     # (B, C)
        return logits

        ## YOUR CODE ENDS HERE ##

    def get_embeddings(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        return self.embedding(x)
        ### YOUR CODE ENDS HERE ###

    def set_embedding_weight(self, weight):
        '''
        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (vocab_size, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.embedding.weight.copy_(weight)
        ### YOUR CODE ENDS HERE ###

    def get_hidden(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        emb  = self.embedding(x)                 # (B, T, D)
        havg = self._masked_average(x, emb)      # (B, D)
        h    = self.relu(self.proj1(havg))       # (B, H)
        return h
        ### YOUR CODE ENDS HERE ###

    def set_hidden_weight(self, weight, bias):
        '''
        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (embedding_dim, hidden_dim)
            bias: torch.tensor of shape (1, hidden_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.proj1.weight.copy_(weight)
            self.proj1.bias.copy_(bias.squeeze(0))
        ### YOUR CODE ENDS HERE ###


def get_dan_model(vocab_size, embedding_dim, hidden_dim):
    """
    This function returns an instance of the DAN model. Initialize the DAN model here and return it. Note that the hidden_dim will be the dimension of the hidden layer in DAN.
    """
    model = None
    ## YOUR CODE STARTS HERE ##
    model = DAN(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_dim=hidden_dim,
        num_classes=len(id2label)   # 20 for this dataset
    )
    ## YOUR CODE ENDS HERE ##
    return model

# Assign hyperparameters and training parameters
# Experiment with different values for these hyperparaters to optimize your model's performance
def get_hyperparams_dan():
  ### your hyper parameters
    learning_rate = 2e-3           # good starting point for Adam
    epochs = 8                     # DAN usually converges in 6–10 epochs
    hidden_layer_dimensions = 256  # can try 128–512; 256 is balanced
    embedding_dim = 200            # 100–300 is common for word embeddings
    ###
    return learning_rate, epochs, hidden_layer_dimensions, embedding_dim



class SimpleAttentionNBOW(nn.Module):
    """
    This class implements the Attention-weighted Neural Bag of Words model.
    """

    def __init__(self, vocab_size, embedding_dim, num_classes=20):
        super().__init__()
        # Do NOT set padding_idx here
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # learnable attention vector u (size d)
        self.att_vector = nn.Parameter(torch.randn(embedding_dim))

        # classifier on top of attention-weighted average
        self.fc = nn.Linear(embedding_dim, num_classes)

    def forward(self, x):
        ## YOUR CODE STARTS HERE ##
        emb = self.embedding(x)                     # (B, T, d)
        alphas = self.get_attention_matrix(x)       # (B, T)
        h_att = torch.einsum("bt,btd->bd", alphas, emb)   # (B, d)
        logits = self.fc(h_att)                     # (B, C)
        return logits
        ## YOUR CODE ENDS HERE ##

        # return predictions

    def get_embeddings(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        return self.embedding(x)
        ### YOUR CODE ENDS HERE ###

    def set_embedding_weight(self, weight):
        '''
        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (vocab_size, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.embedding.weight.copy_(weight)
        ### YOUR CODE ENDS HERE ###

    def set_attention_weights(self, weight):
        '''
        This function sets the attention weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.att_vector.copy_(weight)
        ### YOUR CODE ENDS HERE ###

    def get_attention_matrix(self, x):
        '''
        This function returns the normalized attention matrix for the input x
        Args:
            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch))
        Returns:
            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch))
        '''
        ### YOUR CODE STARTS HERE ###
        emb = self.embedding(x)                     # (B, T, d)
        # cosine similarity between u and each token embedding
        u = F.normalize(self.att_vector, dim=0)     # (d,)
        e = F.normalize(emb, dim=-1) @ u            # (B, T)

        # mask out pad positions (token id == 0)
        mask = (x != 0)                              # (B, T) bool
        e_masked = e.masked_fill(~mask, float('-inf'))

        # normalized attention weights (pad positions -> 0)
        alphas = F.softmax(e_masked, dim=1)
        alphas = alphas * mask.float()
        # (optional) renormalize to ensure sum to 1 over non-pad tokens
        denom = alphas.sum(dim=1, keepdim=True).clamp_min(1e-9)
        alphas = alphas / denom
        return alphas
        ### YOUR CODE ENDS HERE ###


# Assign hyperparameters and training parameters
# Experiment with different values for these hyperparaters to optimize your model's performance
def get_hyperparams_simple_attention():
  ### your hyper parameters
    learning_rate = 3e-3
    epochs = 6
    embedding_dim = 200
    return learning_rate, epochs, embedding_dim

def get_simple_attention_model(vocab_size, embedding_dim):
    """
    This function returns an instance of the SimpleAttentionNBOW model. Initialize the SimpleAttentionNBOW model here and return it.
    """
    model = None
    ## YOUR CODE STARTS HERE ##
    model = SimpleAttentionNBOW(vocab_size=vocab_size, embedding_dim=embedding_dim)
    ## YOUR CODE ENDS HERE ##
    return model


class MultiHeadAttentionNBOW(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_heads, num_classes=20):
        ## YOUR CODE STARTS HERE ##
        super(MultiHeadAttentionNBOW, self).__init__()
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads

        # (assumes PADDING_VALUE is defined globally; value 0 in your notebook)
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PADDING_VALUE)

        # Learnable head vectors u_h (num_heads, embedding_dim)
        self.attn_heads = nn.Parameter(torch.randn(num_heads, embedding_dim))

        # Classifier over an aggregated (B, D) sentence vector
        self.fc = nn.Linear(embedding_dim * num_heads, num_classes, bias=False)
        ## YOUR CODE ENDS HERE ##

    # helper
    def _masked_softmax_over_time(self, scores, mask):
        """
        scores: (B, L, H)
        mask:   (B, L) with True for valid tokens
        returns softmax over L with mask -> (B, L, H)
        """
        # Put -inf where masked so they get zero prob after softmax
        scores = scores.masked_fill(~mask.unsqueeze(-1), float("-inf"))
        return torch.softmax(scores, dim=1)

    # helper
    def _attention_weights(self, x, emb=None):
        """
        x:   (B, L)
        emb: (B, L, D) optional (saves a re-embed)
        returns attn: (B, L, H)
        """
        if emb is None:
            emb = self.embedding(x)                       # (B, L, D)

        # Cosine similarity: normalize both sides
        emb_n = F.normalize(emb, p=2, dim=-1)             # (B, L, D)
        heads_n = F.normalize(self.attn_heads, p=2, dim=-1)  # (H, D)

        # scores_{b,l,h} = <emb_{b,l,:}, head_{h,:}>
        scores = torch.einsum("bld,hd->blh", emb_n, heads_n)  # (B, L, H)

        mask = (x != PADDING_VALUE)                       # (B, L)
        attn = self._masked_softmax_over_time(scores, mask)   # (B, L, H)
        # zero-out any residual numerics on padded positions
        attn = attn * mask.unsqueeze(-1)
        return attn

    def forward(self, x):
        ## YOUR CODE STARTS HERE ##
        # (B, L, D)
        emb = self.embedding(x)

        # (B, L, H) – uses self.attn_heads and masking correctly
        attn = self._attention_weights(x, emb)

        # context per head: (B, H, D)
        ctx = torch.einsum('blh,bld->bhd', attn, emb)

        # CONCATENATE heads (not average): (B, H*D)
        sent_vec = ctx.reshape(ctx.size(0), -1)

        # classifier has no bias; weight = 0.3 in the sanity test
        logits = self.fc(sent_vec)  # (B, num_classes)
        return logits
        ## YOUR CODE ENDS HERE ##


    def get_embeddings(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        return self.embedding(x)
        ### YOUR CODE ENDS HERE ###

    def set_embedding_weight(self, weight):
        '''
        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (vocab_size, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.embedding.weight.copy_(weight)
        ### YOUR CODE ENDS HERE ###

    def set_attention_weights(self, weight):
        '''
        This function sets the attention weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (num_heads, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        if weight.shape != self.attn_heads.shape:
            raise ValueError(f"Expected weight shape {self.attn_heads.shape}, got {weight.shape}")
        with torch.no_grad():
            self.attn_heads.copy_(weight)
        ### YOUR CODE ENDS HERE ###

    def get_attention_matrix(self, x):
        '''
        This function returns the normalized attention matrix for the input x
        Args:
            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch))
        Returns:
            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch, num_heads))
        '''
        ### YOUR CODE STARTS HERE ###
        emb = self.embedding(x)                            # (B, L, D)
        return self._attention_weights(x, emb)             # (B, L, H)
        ### YOUR CODE ENDS HERE ###

# Assign hyperparameters and training parameters
# Experiment with different values for these hyperparaters to optimize your model's performance
def get_hyperparams_multihead():
    learning_rate = 2e-3
    epochs = 8
    num_heads = 4
    embedding_dim = 128
    return learning_rate, epochs, num_heads, embedding_dim

def get_multihead_attention_model(vocab_size, embedding_dim, num_heads):
    """
    This function returns an instance of the MultiHeadAttentionNBOW model. Initialize the MultiHeadAttentionNBOW model here and return it.
    """
    model = None
    ## YOUR CODE STARTS HERE ##
    model = MultiHeadAttentionNBOW(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        num_heads=num_heads
    )
    ## YOUR CODE ENDS HERE ##
    return model

class SelfAttentionNBOW(nn.Module):

    def __init__(self, vocab_size, embedding_dim, num_classes=20):
        super(SelfAttentionNBOW, self).__init__()
        # YOUR CODE STARTS HERE
        # embeddings with padding index
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PADDING_VALUE)
        # linear classifier over the sentence vector
        self.fc = nn.Linear(embedding_dim, num_classes, bias=False)
        # YOUR CODE ENDS HERE

    def forward(self, x):
        # YOUR CODE STARTS HERE
        emb  = self.embedding(x)                 # (B, L, D)
        mask = (x != PADDING_VALUE)              # (B, L)  True for valid tokens

        # pairwise dot-products (no scaling; matches the assignment spec)
        scores = torch.bmm(emb, emb.transpose(1, 2))          # (B, L, L)

        # zero out contributions from padded "keys" before summing over s
        scores_for_sum = scores.masked_fill(~mask.unsqueeze(1), 0.0)  # (B, L, L)

        # logits for alpha: sum over s, then kill padded "queries"
        alpha_logits = scores_for_sum.sum(dim=2)                        # (B, L)
        alpha_logits = alpha_logits.masked_fill(~mask, float('-inf'))   # (B, L)

        # normalized attention over tokens t
        alpha = torch.softmax(alpha_logits, dim=1)                      # (B, L)

        # sentence vector
        h_self = (alpha.unsqueeze(-1) * emb).sum(dim=1)                 # (B, D)

        lengths = mask.sum(dim=1).clamp_min(1).unsqueeze(-1)            # (B, 1)
        h_avg = (emb * mask.unsqueeze(-1)).sum(dim=1) / lengths         # (B, D)

        # classification logits
        h = h_self + h_avg
        logits = self.fc(h)                                             # (B, C)
        return logits
        # YOUR CODE ENDS HERE
    def get_embeddings(self, x):
        '''
        This function returns the embeddings of the input x
        '''
        ### YOUR CODE STARTS HERE ###
        return self.embedding(x)
        ### YOUR CODE ENDS HERE ###
    def set_embedding_weight(self, weight):
        '''
        This function sets the embedding weights to the input weight ensure you aren't recording gradients for this
        Args:
            weight: torch.tensor of shape (vocab_size, embedding_dim)
        '''
        ### YOUR CODE STARTS HERE ###
        with torch.no_grad():
            self.embedding.weight.copy_(weight)
        ### YOUR CODE ENDS HERE ###
    def get_attention_matrix(self, x):
        '''
        This function returns the normalized attention matrix for the input x
        Args:
            x: torch.tensor of shape (BATCH_SIZE, max seq length in batch)
        Returns:
            attention_weights: torch.tensor of shape (BATCH_SIZE, max seq length in batch, max seq length in batch)
        '''
        ### YOUR CODE STARTS HERE ###
        emb  = self.embedding(x)                 # (B, L, D)
        mask = (x != PADDING_VALUE)              # (B, L)

        scores = torch.bmm(emb, emb.transpose(1, 2))                # (B, L, L)
        scores_for_sum = scores.masked_fill(~mask.unsqueeze(1), 0.) # zero padded keys

        alpha_logits = scores_for_sum.sum(dim=2)                    # (B, L)
        alpha_logits = alpha_logits.masked_fill(~mask, float('-inf'))
        alpha = torch.softmax(alpha_logits, dim=1)                  # (B, L)

        # zero-out pads for readability in tests
        return alpha * mask
        ### YOUR CODE ENDS HERE ###


def get_self_attention_model(vocab_size, embedding_dim):
    """
    This function returns an instance of the Self Attention model. Initialize the Self Attention model here and return it.
    """
    model = None
    ## YOUR CODE STARTS HERE ##
    model = SelfAttentionNBOW(vocab_size=vocab_size, embedding_dim=embedding_dim)
    ## YOUR CODE ENDS HERE ##
    return model

# Assign hyperparameters and training parameters
# Experiment with different values for these hyperparaters to optimize your model's performance
def get_hyperparams_self_attn():
    learning_rate = 2e-4
    epochs = 10
    embedding_dim = 200
    return learning_rate, epochs, embedding_dim


class PerceptronLoss(nn.Module):
    def __init__(self):
        super(PerceptronLoss, self).__init__()

    def forward(self, predictions, labels):
        """
        Calculate the perceptron loss between predictions and labels.

        Args:
            predictions (torch.Tensor): The predictions from the model for a batch of inputs.
                                        Shape should be (batch_size, num_classes).
            labels (torch.Tensor): The ground truth labels for each input in the batch.
                                   Shape should be (batch_size,) with each value between 0 and num_classes-1.

        Returns:
            scalar: The mean perceptron loss for the batch.
        """
        loss = None
        # YOUR CODE STARTS HERE
        # Get the score for the true class
        true_scores = predictions[torch.arange(predictions.size(0)), labels]  # shape: (batch_size,)

        # Get the maximum score among *all* classes for each sample
        max_scores, _ = torch.max(predictions, dim=1)  # shape: (batch_size,)

        # Perceptron loss per sample: max(0, max_j(s_j) - s_true)
        losses = torch.clamp(max_scores - true_scores, min=0)

        # Take mean across the batch
        loss = losses.mean()
        # YOUR CODE ENDS HERE
        return loss

class HingeLoss(nn.Module):
    def __init__(self, cost_matrix, device):
        super(HingeLoss, self).__init__()
        """
        cost_matrix is a 2D list. Convert it to a tensor on appropriate device.
        """
        # YOUR CODE STARTS HERE
        self.cost_matrix = torch.tensor(cost_matrix, dtype=torch.float32, device=device)
        # YOUR CODE ENDS HERE

    def forward(self, predictions, labels):
        """
        Calculate the hinge loss between predictions and labels, adjusting for cost.

        Args:
            predictions (torch.Tensor): The predictions from the model for a batch of inputs.
                                        Shape should be (batch_size, num_classes).
            labels (torch.Tensor): The ground truth labels for each input in the batch.
                                   Shape should be (batch_size,) with each value between 0 and num_classes-1.

        Returns:
            scalar: The mean hinge loss for the batch, adjusted for the defined cost.
        """
        loss = None
        # YOUR CODE STARTS HERE
        batch_size, num_classes = predictions.shape

        # Extract the true class scores for each sample
        true_scores = predictions[torch.arange(batch_size), labels].unsqueeze(1)  # shape: (B, 1)

        # Get cost for each (true_label, j) pair
        cost_values = self.cost_matrix[labels]  # shape: (B, C)

        # Compute per-class hinge margin: s_j + cost(j, true) - s_true
        margins = predictions + cost_values - true_scores

        # For each sample, find the maximum margin across all classes
        max_margin, _ = torch.max(margins, dim=1)

        # Apply hinge: max(0, margin)
        hinge_loss = torch.clamp(max_margin, min=0)

        # Average across batch
        loss = hinge_loss.mean()
        # YOUR CODE ENDS HERE
        return loss

def get_cost_matrix(num_classes=20):
    """
    Generates a cost matrix for a specified number of classes using Python lists.

    Args:
        num_classes (int): The number of classes for which the cost matrix is to be created.

    Returns:
        list of lists: A 2D list where element (i, j) is the absolute difference between i and j,
                       set to zero if i equals j.
    """
    cost_matrix = None
    # YOUR CODE STARTS HERE
    cost_matrix = [[abs(i - j) if i != j else 0 for j in range(num_classes)]
                   for i in range(num_classes)]
    # YOUR CODE ENDS HERE
    return cost_matrix