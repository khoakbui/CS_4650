{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffbb14288fe2c51",
   "metadata": {},
   "source": [
    "# CS 4650 - Natural Language Processing - HW - 1\n",
    "Georgia Tech, Fall 2025 (Instructor: Kartik Goyal)\n",
    "\n",
    "In this assignment, you will be implementing different evaluation methodologies for Large Language Models (LLMs) across three key tasks: Multiple Choice Questions (MCQ), Machine Translation (MT), and Short Story Generation (SSG). \n",
    "\n",
    "These tasks are intended to be ascending order of \"open-endedness\". \n",
    "In MCQ, there is a clear notion of a correct answer, and the answer space is constrained to finitely many options. \n",
    "In MT, there are infinitely many correct translations, but the final answer must have the same semantic meaning as the reference sentence. \n",
    "In SSG, the final answer can have different semantic meanings, but some collection of generations may be more desirable (e.g. coherent, diverse) than others. \n",
    "\n",
    "This assignment will cover fundamental evaluation metrics, prompt engineering techniques, and analysis of model performance across each of these different tasks. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DO NOT CHANGE the names of any of the files and contents outside the cells where you have to write code.\n",
    "\n",
    "NOTE: DO NOT USE EXTERNAL LIBRARIES FOR THIS ASSIGNMENT OTHER THAN THE ONES ALREADY IMPORTED\n",
    "\n",
    "**DDL: 9/29/25 11:59PM**\n",
    "\n",
    "Note: this assignment might take substential time to finish (coding + running) and the autograder sometimes takes minutes to finish grading as well and could take longer time right before the deadline due to higher traffic. \n",
    "\n",
    "## **We highly recommend you start early!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SiFjTT4jFEIW",
   "metadata": {
    "id": "SiFjTT4jFEIW"
   },
   "source": [
    "The assignment is broken down into 5 Sections. The sections are as follows:\n",
    "\n",
    "| Section | Part                                      | Points |\n",
    "|---------|-------------------------------------------|--------|\n",
    "| 0       | Setup                                     | 0      |\n",
    "| 1       | Utility Classes                           | 6      |\n",
    "| 2       | Multiple Choice Questions (MCQ) Evaluation| 15     |\n",
    "| 3       | Machine Translation Evaluation            | 14     |\n",
    "| 4       | Short Story Generation Evaluation         | 17     |\n",
    "| 5       | Sampling Hyperparameters and Prompt Optimization (BONUS) | 4 |\n",
    "| Total        |                                        | 50 |\n",
    "| Bonus        |                                        | 8 |\n",
    "\n",
    "<!-- TODO: assign points appropriately. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26764d",
   "metadata": {},
   "source": [
    "## 0. Setup [0 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Check what version of Python is running\n",
    "import os\n",
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e527fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# RUN THIS CELL ONLY IF RUNNING ON GOOGLE COLAB\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U datasets pyarrow fsspec\n",
    "# REPLACE the path to folder where the notebook is located\n",
    "# %cd /content/drive/My\\ Drive/path/to/your/notebook/folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# RUN THIS CELL ONLY IF RUNNING ON PACE-ICE\n",
    "\n",
    "\n",
    "# override the huggingface cache path and nltk cache path\n",
    "dirs = {\n",
    "    \"HF_HOME\":\"~/scratch/hf_cache\",\n",
    "    \"TRITON_CACHE_DIR\":\"~/scratch/triton_cache\",\n",
    "    \"TORCHINDUCTOR_CACHE_DIR\":\"~/scratch/inductor_cache\",\n",
    "    'NLTK_DATA':\"~/scratch/nltk_data\"\n",
    "}\n",
    "\n",
    "for name in dirs:\n",
    "    d = dirs[name]\n",
    "    path = os.path.expanduser(d)\n",
    "    print(name)\n",
    "    print(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    # making sure the cache dirs are rwx for owner\n",
    "    os.chmod(path, 0o700)\n",
    "    os.environ[name] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e55cf463e82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "# Importing required libraries - DO NOT CHANGE THIS CELL\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Sequence, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = 'Your_HF_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Login to Huggingface Hub to access gated models\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Get the Huggingface token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    # Login to Huggingface Hub with the token\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Huggingface Hub\")\n",
    "else:\n",
    "    print(\"Warning: HF_TOKEN environment variable not found. You may not be able to access gated models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Import tokenizer for n-gram matching \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d8b77a802b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "# Defining global constants - DO NOT CHANGE THESE VALUES\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "PADDING_VALUE = 0\n",
    "UNK_VALUE     = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we select a GPU if it's available on your computer or in the Colab environment.\n",
    "print('Device of execution - ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360462",
   "metadata": {},
   "source": [
    "## 1. Utility Classes [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f556d19",
   "metadata": {},
   "source": [
    "In this section, you will implement wrappers around the Huggingface transformers and SentenceTransformers APIs so that they are easier to use later in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiUlTSBB9Wx6",
   "metadata": {},
   "source": [
    "### 1.1. LLM Wrapper Class [5 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1b130",
   "metadata": {},
   "source": [
    "In this section, we'll implement a wrapper around the Huggingface transformers API. \n",
    "\n",
    "The following are provided for you: \n",
    "\n",
    "- Methods to load the tokenizer and model from Huggingface \n",
    "- The `generate()` method to produce text completions from the LLM\n",
    "- The `perplexity()` method for computing \"how confused a model is\" for a given piece of text. (See section 4.2 for details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fac5fa",
   "metadata": {},
   "source": [
    "You will implement the `logits()` method. We will need this for the MCQ task.\n",
    "\n",
    "Logits are the raw, unnormalized scores output by the last layer of the language model.\n",
    "These scores represent the model's prediction for which token in the vocabulary is most likely to follow after the input text.\n",
    "\n",
    "The `logits()` method will extract these raw scores from the model. For multiple-choice questions (MCQ), we select the answer by calculating the logit (either for all the vocabulary items or for the given answer choices) and choosing the most likely one out of all of them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctNnE1Ui8oKw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "@dataclass\n",
    "class LLMGenerationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for LLM generation parameters.\n",
    "    This is for convenience for keeping track of the generation parameters.\n",
    "    http://brainiac:9657/tree?token=9d0d4afb1bc218c5d85f6418e7d1e8007ef0dc1b2a44bd28\n",
    "    Args:\n",
    "        temperature (float): Controls randomness in sampling. Higher values make output more random.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "    \"\"\"\n",
    "    temperature: float = 0.7\n",
    "    max_new_tokens: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqtdrhF8FEIZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    A wrapper class for Hugging Face language models that provides a unified interface\n",
    "    for text generation, logit computation, and perplexity calculation.\n",
    "    \n",
    "    If transformers library is not available, falls back to deterministic stubs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_id: str = \"gpt2\", device: str = None, quantize: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the LLM wrapper.\n",
    "        \n",
    "        Args:\n",
    "            hf_id (str): Hugging Face model identifier\n",
    "            device (str): Device to load model on ('cuda', 'cpu', 'mps')\n",
    "        \"\"\"      \n",
    "        self.hf_id = hf_id\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Use auto-loading with device_map=\"auto\" for faster loading and automatic memory management\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(hf_id)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        if device == 'cpu':\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                hf_id,\n",
    "                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory\n",
    "                load_in_8bit=False\n",
    "            ).to(self.device).eval()\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                hf_id,\n",
    "                device_map=\"auto\",  # Automatically determine optimal device mapping\n",
    "                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory\n",
    "                load_in_8bit=quantize,  # Enable 4-bit quantization for even more memory efficiency\n",
    "            ).eval()\n",
    "                \n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, task_type: str = \"mcq\", config: LLMGenerationConfig = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate text continuation for the given prompt using the underlying language model.\n",
    "        \n",
    "        This method takes a text prompt and generates additional text that continues from\n",
    "        the prompt in a coherent manner. The generation process can be controlled through\n",
    "        various parameters specified in the config object.\n",
    "        \n",
    "        The method tokenizes the input prompt, passes it through the model, and then\n",
    "        decodes the generated token IDs back to text, excluding the original prompt tokens.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt that the model will continue from\n",
    "            config (LLMGenerationConfig): Configuration object containing generation parameters\n",
    "                such as temperature, top_p, top_k, and max_new_tokens. If None, default\n",
    "                parameters will be used.\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated text continuation without the original prompt. The text is\n",
    "                stripped of leading/trailing whitespace and special tokens are removed\n",
    "                during decoding.\n",
    "        \"\"\"\n",
    "\n",
    "        # set generation config \n",
    "        assert task_type in [\"mcq\", \"mt\", \"ssg\"], \"Invalid task_type. Must be one of: mcq, mt, ssg\"\n",
    "        \n",
    "        if config is not None: \n",
    "            # Use the provided config\n",
    "            pass\n",
    "        elif task_type == \"mcq\":\n",
    "            config = LLMGenerationConfig(\n",
    "                temperature=0.0,\n",
    "                max_new_tokens=4,\n",
    "            )\n",
    "        elif task_type == \"mt\":\n",
    "            config = LLMGenerationConfig(\n",
    "                temperature=0.0,\n",
    "                max_new_tokens=200, # max llama=189, qwen=188 on test set; mt contains longer sentences\n",
    "            )\n",
    "        elif task_type == \"ssg\":\n",
    "            config = LLMGenerationConfig(\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=100, # max llama=76, qwen=75 on all five sentences\n",
    "                )   \n",
    "            \n",
    "        \n",
    "\n",
    "        # tokenize the prompt   \n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # generate the output\n",
    "        if config.temperature > 0:\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True, \n",
    "                temperature=config.temperature,\n",
    "                max_length=input_ids.shape[1] + config.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        else:\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                do_sample=False, \n",
    "                max_length=input_ids.shape[1] + config.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(\n",
    "            output[0, input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return generated.strip()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def logits(self, prompt: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get next-token logits for the given prompt.\n",
    "        \n",
    "        This method computes and returns the logits (raw, unnormalized prediction scores) \n",
    "        for the next token that would follow the given prompt. These logits represent the model's\n",
    "        prediction distribution over the entire vocabulary for the next token position.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Input text prompt for which to compute next-token predictions\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (vocab_size,) containing the logits for each\n",
    "                possible next token in the vocabulary. Higher values indicate tokens the\n",
    "                model considers more likely to follow the prompt.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.model(**tokens)\n",
    "        # Return logits for the last token position\n",
    "\n",
    "\n",
    "        #### YOUR CODE HERE #### \n",
    "        \n",
    "        pass\n",
    "        \n",
    "        #### END YOUR CODE #### \n",
    "\n",
    "\n",
    "\n",
    "    ### DO NOT CHANGE THIS FUNCTION ###\n",
    "    @torch.inference_mode()\n",
    "    def perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate perplexity of the given text.\n",
    "        \n",
    "        Perplexity is a measurement of how well a probability model predicts a sample.\n",
    "        Lower perplexity indicates the model is better at predicting the text.\n",
    "        It is calculated as the exponentiated average negative log-likelihood of a sequence.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text for which to calculate perplexity\n",
    "        Returns:\n",
    "            float: Perplexity value\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return 100.0  # Fixed stub value\n",
    "\n",
    "        encodings = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Shift logits and labels for next-token prediction\n",
    "        shift_logits = logits[:, :-1].contiguous()\n",
    "        shift_labels = encodings.input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        return np.exp(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbd769",
   "metadata": {},
   "source": [
    "In the following cells, let's load two different LLMs and test each of the methods we implemented in the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "llama = LLM(hf_id=\"meta-llama/Llama-3.1-8B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79963dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Let's test the generate method using greedy decoding \n",
    "assert llama.device.type == \"cuda\", \"Device is not loaded to cuda\"\n",
    "assert llama.generate(prompt, task_type=\"mcq\") == \"I am doing well\", \"Greedy decoding is incorrect\"\n",
    "\n",
    "assert llama.logits(prompt).shape[0] == 128256, \"Logit shape is incorrect\"\n",
    "assert torch.argmax(llama.logits(prompt)) == 358, \"Logit is incorrect\"\n",
    "assert np.isclose(llama.perplexity(prompt), 17.969428099556087, atol=1e-1), \"Perplexity is incorrect\"\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "qwen = LLM(hf_id=\"Qwen/Qwen2.5-7B-Instruct\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Let's test the generate method using greedy decoding \n",
    "# assert qwen.device.type == \"cuda\", \"Device is not loaded to cuda\" # optional\n",
    "assert qwen.generate(prompt, task_type=\"mcq\") == \"I'm sorry,\", \"Greedy decoding is incorrect\"\n",
    "\n",
    "assert qwen.logits(prompt).shape[0] == 152064, \"Logit shape is incorrect\"\n",
    "assert torch.argmax(qwen.logits(prompt)) == 358, \"Logit is incorrect\"\n",
    "# x = qwen.perplexity(prompt)\n",
    "# print(x)\n",
    "assert np.isclose(qwen.perplexity(prompt), 7.353064664668671, atol=1e-1), \"Perplexity is incorrect\"\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b94263",
   "metadata": {},
   "source": [
    "### 1.2. Embedding Model [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ff39e",
   "metadata": {},
   "source": [
    "Next, we will implement a wrapper around the Huggingface SentenceTransformer API for generating embeddings. \n",
    "Embedding models convert text into dense vector representations (embeddings) that capture semantic meaning.\n",
    "These vectors allow us to measure similarity between texts in a high-dimensional space.\n",
    "#\n",
    "Key points about embedding models:\n",
    "1. They transform variable-length text inputs into fixed-dimension vectors\n",
    "2. They have a maximum context length, so longer inputs will be truncated\n",
    "3. Similar texts will have embeddings that are close to each other in the vector space\n",
    "#\n",
    "We'll use these embeddings when evaluating LLM outputs based on semantic similarity rather than\n",
    "exact string matching, which is particularly useful for tasks like machine translation and\n",
    "short story generation where multiple valid outputs are possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"\n",
    "    A wrapper around the Huggingface SentenceTransformer API for generating embeddings.\n",
    "    This model creates semantic embeddings that can be used for measuring similarity\n",
    "    between texts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_id: str = \"sentence-transformers/all-MiniLM-L6-v2\", dim: int = None):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model with the specified model ID.\n",
    "        \n",
    "        Args:\n",
    "            hf_id (str): Hugging Face model ID for the SentenceTransformer model.\n",
    "                         Default is \"sentence-transformers/all-MiniLM-L6-v2\".\n",
    "            dim (int): Not used for SentenceTransformer models as the dimension is\n",
    "                       determined by the model itself, but kept for API compatibility.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(hf_id,  trust_remote_code=True)\n",
    "        self.dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    def embed(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create an embedding for the given text using the SentenceTransformer model.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to embed. Can be of any length.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Embedding vector representing the semantic content of the input text.\n",
    "        \"\"\"\n",
    "        # SentenceTransformer returns numpy array, convert to torch tensor\n",
    "        embedding = self.model.encode(text, convert_to_tensor=True)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7753b",
   "metadata": {},
   "source": [
    "LaBSE (Language-agnostic BERT Sentence Embedding) is a multilingual embedding model specifically trained for machine translation tasks. It can encode sentences from 109 different languages into a shared embedding space, allowing for effective cross-lingual similarity comparison. This makes it particularly useful for evaluating machine translation outputs by measuring semantic similarity between translations and references, rather than relying on exact string matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96842c4",
   "metadata": {},
   "source": [
    "In the following cells, generate embeddings for the given sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef71f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL \n",
    "\n",
    "labse = EmbeddingModel(hf_id=\"sentence-transformers/LaBSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c821131",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Hello, how are you?\"\n",
    "sentence2 = \"Goodbye, see you later!\"\n",
    "sentence3 = \"What is the weather like today?\"\n",
    "\n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "\n",
    "pass\n",
    "\n",
    "### END YOUR CODE ### \n",
    "\n",
    "\n",
    "assert embedding1.shape == embedding2.shape\n",
    "assert embedding1.shape == (768,)\n",
    "assert embedding2.shape == (768,)\n",
    "assert embedding3.shape == (768,)\n",
    "\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding2, dim=0), torch.tensor(0.4207), atol=1e-1)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding1, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding2, embedding2, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619cd58",
   "metadata": {},
   "source": [
    "The following is an embedding model from Alibaba's GTE (General Text Embedding) family. \n",
    "It is a multilingual embedding model supporting 70 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964936e",
   "metadata": {},
   "source": [
    "In the following cells, generate sentences for the given sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL \n",
    "\n",
    "gte = EmbeddingModel(hf_id=\"Alibaba-NLP/gte-multilingual-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ### \n",
    "\n",
    "pass\n",
    "\n",
    "### END YOUR CODE ### \n",
    "\n",
    "\n",
    "assert embedding1.shape == embedding2.shape\n",
    "assert embedding1.shape == (768,)\n",
    "assert embedding2.shape == (768,)\n",
    "assert embedding3.shape == (768,)\n",
    "\n",
    "\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding2, dim=0), torch.tensor(0.5831), atol=1e-1)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding1, embedding1, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "assert torch.isclose(torch.cosine_similarity(embedding2, embedding2, dim=0), torch.tensor(1.0), atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDI72x8XFEIc",
   "metadata": {},
   "source": [
    "## 2. Multiple Choice Questions (MCQ) Evaluation [9 points Programming + 4 points Non-Programming + 2 BONUS points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0dd78",
   "metadata": {},
   "source": [
    "In this secton, we'll be evaluating LLMs on the [MMLU (Massive Multitask Language Understanding)](https://arxiv.org/abs/2009.03300), a benchmark dataset consisting of multiple choice questions. \n",
    "It covers a wide range of subjects including mathematics, history, computer science, law, and more, making it a comprehensive test of an LLM's knowledge and reasoning abilities.\n",
    "MCQ benchmarks are usually reported using accuracies which simply mean how many questions the model answered correctly. \n",
    "\n",
    "We'll implement two different evaluation approaches:\n",
    "\n",
    "1. **Regex-based Accuracy**: A simple approach that extracts answers from model outputs using regular expressions, then compares them to ground truth answers.\n",
    "\n",
    "2. **Logit-based Accuracy**: A more sophisticated approach that directly accesses the model's internal probability distributions (logits) to determine which answer choice the model predicts as most likely.\n",
    "\n",
    "Each approach has its advantages and limitations, which we'll explore throughout this section.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e50b95",
   "metadata": {},
   "source": [
    "### 2.1. Regex-based Accuracy [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e55f04",
   "metadata": {},
   "source": [
    "In the following cell, implement a regular expression (regex) which extracts the first letter generated by the LLM and compares it with the reference answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fCFfEHv1hnI",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "\n",
    "_mcq_regex = None\n",
    "\n",
    "### END YOUR CODE ### \n",
    "\n",
    "def compute_regex_accuracy(generation: str, reference: str) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Extract the first letter A-D from the generation and compare with reference.\n",
    "    \n",
    "    Args:\n",
    "        generation (str): Generated text from the model\n",
    "        reference (str): Ground truth answer (A, B, C, or D)\n",
    "        \n",
    "    Returns:\n",
    "        float: 1.0 if correct, 0.0 if incorrect\n",
    "        str: The predicted letter (A, B, C, or D)\n",
    "    \"\"\"\n",
    "    match = _mcq_regex.search(generation)\n",
    "    regex_pred = match.group(1).upper() if match else None\n",
    "    return float(regex_pred == reference.strip().upper()), regex_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb94a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = \"The answer is B\"\n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_regex_accuracy(generation, reference)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"B\"\n",
    "print(f\"Test passed - Basic correct\")\n",
    "\n",
    "generation = \"the answer is clearly d\"\n",
    "reference = \"D\"\n",
    "accuracy, pred = compute_regex_accuracy(generation, reference)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"D\"\n",
    "print(f\"Test passed - Lowercase\")\n",
    "\n",
    "generation = \"The correct option is (B)\"\n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_regex_accuracy(generation, reference)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"B\"\n",
    "print(f\"Test passed - Answer in parentheses\")\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8-qTQQa2FEIe",
   "metadata": {},
   "source": [
    "### 2.2. Logit-based Accuracy. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1382c3",
   "metadata": {},
   "source": [
    "In the following cell, implement a function which calculates the logit-based accuracy for a single question using the logits returned by the model. \n",
    "That is, given the logits of the model, find the \n",
    "\n",
    "The function should support an optional `valid_letters` argument such that, \n",
    "when a list is passed to it, the maximum is computed over these valid letters (as opposed to, over the entire vocabulary).\n",
    "For example, if the list `[A,B,C,D]` is passed to the function, then we will get the accuracy based on which letters the model thinks is most likely *out of the four letters contained in `valid_letters`*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqt9q92J1QKK",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def compute_logit_accuracy(logits: torch.Tensor, reference: str, model: LLM, valid_letters: List[str] = None) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Compute accuracy based on logits for the first token generated by the LLM.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits from the model for the first token\n",
    "        reference (str): Ground truth answer (A, B, C, or D)\n",
    "        model (LLM): The language model used for generation, needed for tokenizer access\n",
    "        valid_letters (List[str]): List of valid letters to restrict argmax to (e.g., ['A', 'B', 'C', 'D'])\n",
    "                                If None, uses the entire vocabulary\n",
    "        \n",
    "    Returns:\n",
    "        float: 1.0 if the token with highest logit matches reference, 0.0 otherwise\n",
    "        str: The predicted letter (If `valid_letters` is None, then the predicted letter is not constrained to be one of the valid letters, but can be any letter in the vocabulary)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE ### \n",
    "    \n",
    "    if valid_letters is not None:\n",
    "        # limit vocabulary to only the valid letters\n",
    "        pass\n",
    "    else:\n",
    "        # use entire vocabulary\n",
    "        pass\n",
    "    \n",
    "    # compute accuracy\n",
    "    pass\n",
    "\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "llama_token_ids = {\n",
    "    letter: llama.tokenizer.encode(letter, add_special_tokens=False)[0] \n",
    "    for letter in LETTERS\n",
    "}\n",
    "\n",
    "qwen_token_ids = {\n",
    "    letter: qwen.tokenizer.encode(letter, add_special_tokens=False)[0] \n",
    "    for letter in LETTERS\n",
    "}\n",
    "\n",
    "vocab_size = len(llama.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[llama_token_ids[\"B\"]] = 15.0  \n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"B\"\n",
    "print(\"Test passed - llama, correct answer\")\n",
    "\n",
    "vocab_size = len(llama.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[2196] = 20.0 \n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama)\n",
    "assert accuracy == 0.0\n",
    "assert pred == \"context\"\n",
    "print(\"Test passed - llama, wrong answer\")\n",
    "\n",
    "\n",
    "vocab_size = len(llama.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[2196] = 20.0 \n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama)\n",
    "assert accuracy == 0.0\n",
    "assert pred == \"context\"\n",
    "print(\"Test passed - llama, wrong answer\")\n",
    "\n",
    "\n",
    "vocab_size = len(llama.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[llama_token_ids[\"C\"]] = 15.0  \n",
    "reference = \"C\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama, LETTERS)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"C\"\n",
    "print(\"Test passed - llama, valid letter C\")\n",
    "\n",
    "\n",
    "vocab_size = len(llama.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[llama_token_ids[\"A\"]] = 15.0  \n",
    "reference = \"A\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama, LETTERS)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"A\"\n",
    "print(\"Test passed - llama, valid letter A\")\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(qwen.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[qwen_token_ids[\"D\"]] = 15\n",
    "reference = \"D\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, qwen)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"D\"\n",
    "print(\"Test passed - qwen, correct answer\")\n",
    "\n",
    "vocab_size = len(qwen.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0\n",
    "logits[670] = 20.0  \n",
    "reference = \"A\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, llama)\n",
    "assert accuracy == 0.0\n",
    "assert pred == \"uct\"\n",
    "print(\"Test passed - qwen, wrong answer\")\n",
    "\n",
    "\n",
    "vocab_size = len(qwen.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[qwen_token_ids[\"A\"]] = 15.0  \n",
    "reference = \"A\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, qwen, LETTERS)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"A\"\n",
    "print(\"Test passed - qwen, valid letter A\")\n",
    "\n",
    "vocab_size = len(qwen.tokenizer)\n",
    "logits = torch.randn(vocab_size) * 2.0  \n",
    "logits[qwen_token_ids[\"B\"]] = 15.0  \n",
    "reference = \"B\"\n",
    "accuracy, pred = compute_logit_accuracy(logits, reference, qwen, LETTERS)\n",
    "assert accuracy == 1.0\n",
    "assert pred == \"B\"\n",
    "print(\"Test passed - qwen, valid letter B\")\n",
    "\n",
    "del logits\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OayoJRTeFEIf",
   "metadata": {},
   "source": [
    "### 2.3. Run MCQ Evaluation [2 points - Non Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97736d3",
   "metadata": {},
   "source": [
    "In the following cells run the evaluation on the LLMs with the MMLU benchmark.\n",
    "In the final cell, comment on what you observe from the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9439a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Load MMLU dataset from Huggingface\n",
    "mmlu_test_raw = load_dataset(\"lighteval/mmlu\", \"high_school_geography\", split=\"test\")\n",
    "\n",
    "# preprocess\n",
    "mmlu_test_data = []\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mmlu_test_raw:\n",
    "    eval_item = {\n",
    "        \"question\": item[\"question\"], \n",
    "        \"option_A\": item[\"choices\"][0],\n",
    "        \"option_B\": item[\"choices\"][1],\n",
    "        \"option_C\": item[\"choices\"][2],\n",
    "        \"option_D\": item[\"choices\"][3],\n",
    "        \"reference\": LETTERS[item[\"answer\"]],\n",
    "        \"reference_idx\": item[\"answer\"]\n",
    "    }\n",
    "    mmlu_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "llama_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(mmlu_test_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = llama.generate(prompt, config=config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    llama_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama)\n",
    "    llama_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "    # compute logit accuracy when restricted to valid letters\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama, LETTERS)\n",
    "    llama_mmlu_scores[\"logit_accuracy_restricted\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in llama_mmlu_scores.items():\n",
    "    numeric_scores = [score for score, _ in metric_scores]\n",
    "    print(f\"{metric_name}: {np.mean(numeric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f660f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "qwen_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(mmlu_test_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = qwen.generate(prompt, config=config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    qwen_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen)\n",
    "    qwen_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "    # compute logit accuracy when restricted to valid letters\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen, LETTERS)\n",
    "    qwen_mmlu_scores[\"logit_accuracy_restricted\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in qwen_mmlu_scores.items():\n",
    "    numeric_scores = [score for score, _ in metric_scores]\n",
    "    print(f\"{metric_name}: {np.mean(numeric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8c06b",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc772d2",
   "metadata": {},
   "source": [
    "### 2.5  \"None of the Above\" [2 points Programming + 2 point Non Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b670efc",
   "metadata": {},
   "source": [
    "In this evaluation section, we assess the model's ability to detect when the correct answer is missing from a set of provided optionsâ€”a key aspect of robust question-answering systems.\n",
    "\n",
    "Your tasks are as follows:\n",
    "\n",
    "1. Create a modified version of the MMLU dataset by randomly replacing one answer choice with 'None of the above'.\n",
    "2. Record which questions have had their true correct answer replaced, as these will serve as the ground truth for when 'None of the above' is correct.\n",
    "3. Evaluate the model's performance using precision, recall, and F1-score:\n",
    "\n",
    "    **Precision** measures the accuracy when the model selects 'None of the above':\n",
    "    $$\\mathrm{Precision} = \\frac{\\mathrm{True\\ Positives}}{\\mathrm{True\\ Positives} + \\mathrm{False\\ Positives}}$$\n",
    "\n",
    "    **Recall** quantifies how often the model correctly identifies cases where the correct answer is missing:\n",
    "    $$\\mathrm{Recall} = \\frac{\\mathrm{True\\ Positives}}{\\mathrm{True\\ Positives} + \\mathrm{False\\ Negatives}}$$\n",
    "\n",
    "    **F1-score**, the harmonic mean of precision and recall, offers a balanced performance measure:\n",
    "    $$\\mathrm{F1} = 2 \\cdot \\frac{\\mathrm{Precision} \\cdot \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}$$\n",
    "\n",
    "For clarity, the evaluation categories are defined as:\n",
    "\n",
    "- **True Positive (TP)**: The model selects 'None of the above' when the correct answer was replaced.\n",
    "- **False Positive (FP)**: The model selects 'None of the above' even though the correct answer remains among the options.\n",
    "- **True Negative (TN)**: The model selects an option other than 'None of the above' when the original correct answer is still available.\n",
    "- **False Negative (FN)**: The model selects an option other than 'None of the above' even though the correct answer was replaced.\n",
    "\n",
    "These metrics will help determine whether the model can correctly identify scenarios where no provided answer is correct. When preparing the `modified_mmlu_test_data`, feel free to include extra keys to capture which questions were modified and how the correct answers were affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbeda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MMLU dataset from Huggingface\n",
    "mmlu_test_raw = load_dataset(\"lighteval/mmlu\", \"high_school_geography\", split=\"test\")\n",
    "\n",
    "LETTERS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# preprocess\n",
    "modified_mmlu_test_data = []\n",
    "\n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "# for each test case, randomly select one of the four options to replace with \"None of the above\"\n",
    "pass\n",
    "\n",
    "### END YOUR CODE ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "\n",
    "# USING LLAMA, for each test case in your modified mmlu test data\n",
    "# calculate the following and store the result for metric calculation later\n",
    "    # 1. compute regex accuracy\n",
    "    # 2. compute regex stats\n",
    "    # 3. compute logit accuracy\n",
    "    # 4. compute logit stats\n",
    "    # 5. compute logit accuracy when restricted to valid letters\n",
    "    # 6. compute logit stats\n",
    "\n",
    "\n",
    "# compute regex precision \n",
    "\n",
    "\n",
    "# compute regex recall \n",
    "\n",
    "\n",
    "# compute regex f1 score\n",
    "\n",
    "\n",
    "# compute logit precision \n",
    "\n",
    "\n",
    "# compute logit recall \n",
    "\n",
    "\n",
    "# compute logit f1 score\n",
    "\n",
    "\n",
    "# compute logit precision when restricted to valid letters\n",
    "\n",
    "\n",
    "# compute logit recall when restricted to valid letters \n",
    "\n",
    "\n",
    "# compute logit f1 score when restricted to valid letters  \n",
    "\n",
    "### END YOUR CODE ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "### YOUR CODE HERE ### \n",
    "# USING QWEN, for each test case in your modified mmlu test data\n",
    "# calculate the following and store the result for metric calculation later\n",
    "    # 1. compute regex accuracy\n",
    "    # 2. compute regex stats\n",
    "    # 3. compute logit accuracy\n",
    "    # 4. compute logit stats\n",
    "    # 5. compute logit accuracy when restricted to valid letters\n",
    "    # 6. compute logit stats\n",
    "\n",
    "\n",
    "# compute regex precision \n",
    "\n",
    "\n",
    "# compute regex recall \n",
    "\n",
    "\n",
    "# compute regex f1 score\n",
    "\n",
    "\n",
    "# compute logit precision \n",
    "\n",
    "\n",
    "# compute logit recall \n",
    "\n",
    "\n",
    "# compute logit f1 score\n",
    "\n",
    "\n",
    "# compute logit precision when restricted to valid letters\n",
    "\n",
    "\n",
    "# compute logit recall when restricted to valid letters \n",
    "\n",
    "\n",
    "# compute logit f1 score when restricted to valid letters  \n",
    "\n",
    "\n",
    "### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4a7b9",
   "metadata": {},
   "source": [
    "In the cell below, explain what you observe for each of the evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d4b80",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613130c",
   "metadata": {},
   "source": [
    "### 2.6 Shuffling Choices. [BONUS - 2 points Non-Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d62e8f",
   "metadata": {},
   "source": [
    "When evaluating LLMs on multiple choice questions, we need to be careful about memorization effects.\n",
    "Memorization occurs when an LLM has seen very similar questions during training,\n",
    "and can simply recall the correct answer rather than reasoning about the question.\n",
    "#\n",
    "For example, if an LLM was trained on practice tests that contained the same multiple choice\n",
    "question with answers in the same order (A, B, C, D), it might memorize that \"A\" was correct without\n",
    "actually understanding the question.\n",
    "#\n",
    "To help distinguish between true reasoning ability and memorization, we can randomly shuffle the order\n",
    "of answer choices while preserving which answer is correct. This way, even if the LLM has seen the\n",
    "question before, it needs to identify the correct answer based on content rather than position.\n",
    "# \n",
    "Re-evaluate the two models in the shuffled case and report the resulting accuracies. Comment on whether the results differ from the accuracies you got for sections 2.4 and 2.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# preprocess\n",
    "shuffled_mmlu_test_data = []\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mmlu_test_raw:\n",
    "    shuffled_indices = list(range(len(item[\"choices\"])))\n",
    "    random.shuffle(shuffled_indices)  # Actually shuffle the indices\n",
    "    \n",
    "    # Find where the original correct answer ended up after shuffling\n",
    "    new_reference_idx = shuffled_indices.index(item[\"answer\"])\n",
    "    \n",
    "    eval_item = {\n",
    "        \"question\": item[\"question\"], \n",
    "        \"option_A\": item[\"choices\"][shuffled_indices[0]],\n",
    "        \"option_B\": item[\"choices\"][shuffled_indices[1]],\n",
    "        \"option_C\": item[\"choices\"][shuffled_indices[2]],\n",
    "        \"option_D\": item[\"choices\"][shuffled_indices[3]],\n",
    "        \"reference\": LETTERS[new_reference_idx],\n",
    "        \"reference_idx\": new_reference_idx\n",
    "    }\n",
    "    shuffled_mmlu_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "shuffled_llama_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(shuffled_mmlu_test_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = llama.generate(prompt, config=config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    shuffled_llama_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama)\n",
    "    shuffled_llama_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "    # compute logit accuracy when restricted to valid letters\n",
    "    logits = llama.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, llama, LETTERS)\n",
    "    shuffled_llama_mmlu_scores[\"logit_accuracy_restricted\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in shuffled_llama_mmlu_scores.items():\n",
    "    numeric_scores = [score for score, _ in metric_scores]\n",
    "    print(f\"{metric_name}: {np.mean(numeric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c696b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "config = LLMGenerationConfig(\n",
    "    temperature=0,\n",
    "    max_new_tokens=5\n",
    ")\n",
    "prompt_template = open(\"prompts/mcq/default.txt\").read()\n",
    "\n",
    "qwen_mmlu_scores = defaultdict(list)\n",
    "for item in tqdm(shuffled_mmlu_test_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"question\"]\n",
    "    reference, reference_idx = item[\"reference\"], item[\"reference_idx\"]\n",
    "    \n",
    "    # compute regex accuracy\n",
    "    hypothesis = qwen.generate(prompt, config=config)\n",
    "    regex_accuracy = compute_regex_accuracy(hypothesis, reference)\n",
    "    qwen_mmlu_scores[\"regex_accuracy\"].append(regex_accuracy)\n",
    "\n",
    "    # compute logit accuracy\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen)\n",
    "    qwen_mmlu_scores[\"logit_accuracy\"].append(logit_accuracy)\n",
    "\n",
    "    # compute logit accuracy when restricted to valid letters\n",
    "    logits = qwen.logits(prompt)\n",
    "    logit_accuracy = compute_logit_accuracy(logits, reference, qwen, LETTERS)\n",
    "    qwen_mmlu_scores[\"logit_accuracy_restricted\"].append(logit_accuracy)\n",
    "\n",
    "\n",
    "for metric_name, metric_scores in qwen_mmlu_scores.items():\n",
    "    numeric_scores = [score for score, _ in metric_scores]\n",
    "    print(f\"{metric_name}: {np.mean(numeric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee95e5",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BWLK7T1uFEIg",
   "metadata": {},
   "source": [
    "## 3. Machine Translation Evaluation [11 points Programming + 3 points Non-Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491813d0",
   "metadata": {},
   "source": [
    "In this section, we'll be evaluating the Llama model on German to English machine translation tasks. In machine translation, models automatically converting text from one language to another while preserving meaning and fluency.\n",
    "\n",
    "We'll implement three different evaluation metrics:\n",
    "\n",
    "1. **N-gram Overlap**: A simple metric that counts the number of matching n-grams (sequences of n consecutive words) between the machine translation and reference translation. It provides a basic measure of lexical similarity.\n",
    "#\n",
    "2. **BLEU Score**: A precision-based metric that measures the overlap of n-grams between the machine translation and reference translations. It's one of the most widely used metrics in machine translation evaluation.\n",
    "#\n",
    "3. **Embedding-based Similarity**: Instead of direct word matching, we use an embedding model to convert sentences into vector representations that capture semantic meaning. \n",
    "\n",
    "Each metric has its strengths and limitations, which we'll explore throughout this section. We'll use these metrics to evaluate the translation capabilities of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7101",
   "metadata": {},
   "source": [
    "### 3.1. Helper Functions [2 points Programming]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e271f",
   "metadata": {},
   "source": [
    "In this section, we define some useful helper functions for evaluating the LLMs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac52ecf",
   "metadata": {},
   "source": [
    "N-grams are contiguous sequences of n items from a given sample of text. \n",
    "For example, in the sentence \"I love natural language processing\":\n",
    "- 1-grams (unigrams): [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "- 2-grams (bigrams): [(\"I\", \"love\"), (\"love\", \"natural\"), (\"natural\", \"language\"), (\"language\", \"processing\")]\n",
    "- 3-grams (trigrams): [(\"I\", \"love\", \"natural\"), (\"love\", \"natural\", \"language\"), (\"natural\", \"language\", \"processing\")]\n",
    "#\n",
    "N-grams are useful for various NLP tasks including:\n",
    "- Language modeling: Predicting the next word based on previous words\n",
    "- Text similarity: Comparing documents based on shared n-grams\n",
    "- Machine translation evaluation: Metrics like BLEU use n-gram overlap\n",
    "#\n",
    "Implement the `_ngrams` function below which extracts all possible n-grams from a sequence of tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fe487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a text string into a list of tokens. This is only for n-gram matching. \n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text string to tokenize\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of tokens extracted from the input text\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67093abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def _ngrams(tokens: Sequence[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from a sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens (Sequence[str]): A sequence of tokens (words, characters, etc.)\n",
    "        n (int): The size of each n-gram\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[str, ...]]: A list of tuples, where each tuple contains n consecutive tokens\n",
    "                              from the input sequence\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ### \n",
    "    pass\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb6005",
   "metadata": {},
   "source": [
    "### 3.2. N-gram Overlap [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52165f30",
   "metadata": {},
   "source": [
    "N-gram overlap is a simple metric that measures how many n-grams are shared between the generated text and the reference text. n-gram overlap focuses on a single n-gram size and calculates the recall - what fraction of reference n-grams appear in the candidate text.\n",
    "\n",
    "This metric is useful for evaluating:\n",
    "- Content coverage: How much of the reference content is captured in the generation\n",
    "- Lexical similarity: The degree to which the same word sequences are used\n",
    "\n",
    "N-gram overlap is particularly helpful for tasks where we want to ensure specific information from a reference is included in the generated text, such as summarization or story generation.\n",
    "\n",
    "Implement the `compute_ngram_overlap()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def compute_corpus_ngram_overlap(candidates: List[str], references: List[str], n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    Compute corpus-level n-gram overlap by aggregating across all sentences.\n",
    "    Compute the n-gram overlap, accounting for the number of times each n-gram appears in the candidate and reference.\n",
    "    \n",
    "    Args:\n",
    "        candidates (List[str]): List of candidate translations\n",
    "        references (List[str]): List of reference translations  \n",
    "        n (int): N-gram order\n",
    "        \n",
    "    Returns:\n",
    "        float: Corpus-level n-gram overlap score\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ### \n",
    "    # Initialize counters for n-grams\n",
    "    pass\n",
    "    \n",
    "    # Calculate overlap considering frequencies\n",
    "    pass\n",
    "\n",
    "    ### END YOUR CODE ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e91974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "test_text = \"the cat sat on the mat\"\n",
    "tokens = preprocess_text(test_text)\n",
    "print(f\"preprocess_text result: {tokens}\")\n",
    "\n",
    "test_tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "bigrams = _ngrams(test_tokens, 2)\n",
    "print(f\"ngram result, n=2: {bigrams}\")\n",
    "\n",
    "test_tokens = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "trigrams = _ngrams(test_tokens, 3)\n",
    "print(f\"ngram result, n=3: {trigrams}\")\n",
    "\n",
    "# Test - perfect match\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"the cat sat on the mat\"]\n",
    "score = compute_corpus_ngram_overlap(candidates, references, n=2)\n",
    "assert score == 1.0, f\"Expected 1.0, got {score}\"\n",
    "print(\"Test passed - perfect match\")\n",
    "\n",
    "# Test - no match\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"dogs run fast\"]\n",
    "score = compute_corpus_ngram_overlap(candidates, references, n=2)\n",
    "assert score == 0.0, f\"Expected 0.0, got {score}\"\n",
    "print(\"Test passed - no match\")\n",
    "\n",
    "# Test - partial match\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"some dogs also sat on the mat\"]\n",
    "score = compute_corpus_ngram_overlap(candidates, references, n=3)\n",
    "assert score == 0.4, f\"Expected 0.4, got {score}\"\n",
    "print(\"Test passed - partial match\")\n",
    "\n",
    "# Test - multiple sentences\n",
    "candidates = [\"the cat sat\", \"the dog ran\"]\n",
    "references = [\"the cat sat\", \"the dog walked\"]\n",
    "score = compute_corpus_ngram_overlap(candidates, references, n=2)\n",
    "assert torch.isclose(torch.tensor(score), torch.tensor(0.75), atol=0.001), f\"Expected 0.75, got {score}\"\n",
    "print(\"Test passed - multiple sentences\")\n",
    "\n",
    "# Test - repeated n-grams\n",
    "candidates = [\"the the the dog\"]\n",
    "references = [\"the the cat cat\"]\n",
    "score = compute_corpus_ngram_overlap(candidates, references, n=2)\n",
    "assert torch.isclose(torch.tensor(score), torch.tensor(1/3), atol=0.001), f\"Expected 0.3333, got {score}\"\n",
    "print(\"Test passed - repeated n-grams\")\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pZDPs0Sf-H3V",
   "metadata": {},
   "source": [
    "### 3.3. BLEU Score [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cff37",
   "metadata": {},
   "source": [
    "BLEU (Bilingual Evaluation Understudy) is a widely used metric for evaluating machine translation quality. \n",
    "This is done by: \n",
    "\n",
    "- Finding the n-grams for different values of n (typically n = 1,...,4). \n",
    "- Calculating the n-gram precisions between the candidate and reference texts for each n. This is the fraction of matching n-grams between the two texts, out of the n-grams from the candidate text. \n",
    "\n",
    "The final BLEU score is a geometric mean of these precision scores, multiplied by the brevity penalty.\n",
    "\n",
    "The brevity penalty is crucial because without it, systems could artificially inflate their scores by \n",
    "producing very short translations that contain only high-confidence words. For example, a system could \n",
    "output just a few words that are guaranteed to be correct, achieving high precision while omitting \n",
    "much of the content. The brevity penalty addresses this by reducing the score when the candidate \n",
    "translation is shorter than the reference translation.\n",
    "\n",
    "\n",
    "In the following cells, implement the precision calculation in the BLEU score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jzGx2q0jLqyU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def compute_corpus_bleu(candidates: List[str], references: List[str], max_n: int = 4) -> float:\n",
    "    \"\"\"\n",
    "    Compute corpus-level BLEU score by aggregating n-gram statistics across all sentences.\n",
    "    \n",
    "    Args:\n",
    "        candidates (List[str]): List of candidate translations\n",
    "        references (List[str]): List of reference translations\n",
    "        max_n (int): Maximum n-gram order to consider\n",
    "        \n",
    "    Returns:\n",
    "        float: Corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    # Aggregate statistics across all sentences\n",
    "    total_cand_len = 0\n",
    "    total_ref_len = 0\n",
    "    ngram_matches = [0] * max_n\n",
    "    ngram_totals = [0] * max_n\n",
    "    \n",
    "    # iterate over each candidate and reference\n",
    "    for cand, ref in zip(candidates, references):\n",
    "        cand_tokens = preprocess_text(cand)\n",
    "        ref_tokens = preprocess_text(ref)\n",
    "        \n",
    "        total_cand_len += len(cand_tokens)\n",
    "        total_ref_len += len(ref_tokens)\n",
    "        \n",
    "        for n in range(1, max_n + 1):\n",
    "            cand_ngrams = Counter(_ngrams(cand_tokens, n))\n",
    "            ref_ngrams = Counter(_ngrams(ref_tokens, n))\n",
    "            \n",
    "            overlap = {ng: min(count, ref_ngrams[ng]) for ng, count in cand_ngrams.items()}\n",
    "            ngram_matches[n-1] += sum(overlap.values())\n",
    "            ngram_totals[n-1] += sum(cand_ngrams.values())\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    ### YOUR CODE HERE ### \n",
    "\n",
    "    # calculate precision for each n-gram order\n",
    "    pass\n",
    "\n",
    "    ### END YOUR CODE ### \n",
    "    \n",
    "    # Geometric mean with smoothing\n",
    "    eps = 1e-9\n",
    "    geo_mean = np.exp(sum(np.log(p + eps) for p in precisions) / max_n)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = 1.0 if total_cand_len > total_ref_len else np.exp(1 - total_ref_len / max(total_cand_len, 1))\n",
    "    \n",
    "    return bp * geo_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15475acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - perfect match\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"the cat sat on the mat\"]\n",
    "score = compute_corpus_bleu(candidates, references, max_n=4)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(1.0), atol=0.001)\n",
    "print(\"Test passed - perfect match\")\n",
    "\n",
    "# Test - multiple sentences\n",
    "candidates = [\"the cat sat on the mat\", \"the dog ran in the park\"]\n",
    "references = [\"the cat sat on the mat\", \"the dog ran in the garden\"]\n",
    "score = compute_corpus_bleu(candidates, references, max_n=4)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(0.8806841685), atol=0.001)\n",
    "print(\"Test passed - multiple sentences\")\n",
    "\n",
    "# Test - zero score\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"dogs run fast\"]\n",
    "score = compute_corpus_bleu(candidates, references, max_n=4)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(0.0), atol=0.001)\n",
    "print(\"Test passed - zero score\")\n",
    "\n",
    "print(\"All tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f53a1",
   "metadata": {},
   "source": [
    "### 3.4. Embedding-based Similarity [2 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0f0f0",
   "metadata": {},
   "source": [
    "Embedding-based similarity metrics go beyond surface-level text matching by capturing semantic relationships between words and phrases. Unlike n-gram overlap or BLEU, which rely on exact matches, embedding-based methods can recognize when different words express similar meanings.\n",
    "\n",
    "This approach works by:\n",
    "1. Converting texts into dense vector representations (embeddings) using pre-trained models\n",
    "2. Measuring the similarity between these vectors using metrics like cosine similarity\n",
    "3. Producing a score that reflects semantic similarity rather than lexical overlap\n",
    "\n",
    "Unlike BLEU and n-gram overlap which are typically calculated at the corpus level, embedding-based similarity is a sentence-level metric that evaluates each translation independently. This makes it particularly useful for assessing individual translations without requiring a large corpus for reliable scores.\n",
    "\n",
    "Implement the `compute_embedding_similarity()` function below to calculate the semantic similarity between generated and reference texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def compute_embedding_similarity(\n",
    "    generation: str, \n",
    "    reference: str, \n",
    "    embedder: EmbeddingModel = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function computes the semantic similarity between a generated text and a reference text\n",
    "    using embeddings and cosine similarity.\n",
    "    \n",
    "    Embedding-based similarity captures semantic meaning beyond exact word matches, allowing\n",
    "    for evaluation of paraphrases and texts that convey similar meaning with different words.\n",
    "    \n",
    "    Args:\n",
    "        generation (str): Generated text to be evaluated\n",
    "        reference (str): Reference text to compare against\n",
    "        embedder (EmbeddingModel): Model to create text embeddings. If None, a default model is used.\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the embeddings, ranging from -1.0 to 1.0,\n",
    "              where higher values indicate greater semantic similarity\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ### \n",
    "    \n",
    "    pass\n",
    "\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a884ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - perfect match\n",
    "candidates = \"the cat sat on the mat\"\n",
    "references = \"the cat sat on the mat\"\n",
    "score = compute_embedding_similarity(candidates, references, labse)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(1.0), atol=0.001)\n",
    "print(\"Test passed - perfect match\")\n",
    "\n",
    "# Test - paraphrase\n",
    "candidates = \"It is nice today\"\n",
    "references = \"Today the weather is good\"\n",
    "score = compute_embedding_similarity(candidates, references, labse)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(0.82423102856), atol=0.001)\n",
    "print(\"Test passed - paraphrase\")\n",
    "\n",
    "# Test - unrelated topic\n",
    "candidates = [\"the cat sat on the mat\"]\n",
    "references = [\"Today the weather is good\"]\n",
    "score = compute_embedding_similarity(candidates, references, labse)\n",
    "assert torch.isclose(torch.tensor(score, dtype=torch.float32), torch.tensor(0.27818429470), atol=0.001)\n",
    "print(\"Test passed - unrelated topic\")\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2a8d9",
   "metadata": {},
   "source": [
    "### 3.5. Test Machine Translation Metrics [3 points - Non Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a4188",
   "metadata": {},
   "source": [
    "Now let's test our machine translation metrics on a small dataset of German-to-English translations.\n",
    "\n",
    "In this section, we will:\n",
    "1. Load a test dataset containing German source sentences and English reference translations\n",
    "2. Use our LLM to generate English translations from the German source\n",
    "3. Evaluate the translations using the metrics we've implemented \n",
    "\n",
    "This will demonstrate how these metrics can be used to assess machine translation quality.\n",
    "Since the generation process takes quite a while, we will only be running it on Llama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f005d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# load data\n",
    "\n",
    "source_text = open(\"data/mt/test.de-en.de\", 'r').read().split(\"\\n\")\n",
    "target_text = open(\"data/mt/test.de-en.en\", 'r').read().split(\"\\n\")\n",
    "\n",
    "mt_test_raw = list(zip(source_text, target_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cf1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# preprocess\n",
    "mt_test_data = []\n",
    "\n",
    "# See: `prompts/mcq/default.txt` for the prompt template format \n",
    "for item in mt_test_raw:\n",
    "    eval_item = {\n",
    "        \"source_language\": \"de\",\n",
    "        \"target_language\": \"en\",\n",
    "        \"source_text\": item[0],\n",
    "        \"target_text\": item[1]\n",
    "    }\n",
    "    mt_test_data.append(eval_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "def extract_translation(completion):\n",
    "    \"\"\"\n",
    "    Extract the translation from the completion.\n",
    "    The function looks for text after the '[en]:' tag until it reaches:\n",
    "    - Another language tag (e.g., [de]:, [fr]:)\n",
    "    - A period followed by a note in parentheses\n",
    "    - The end of the string\n",
    "    Args:\n",
    "        completion (str): The completion text from the LLM\n",
    "    Returns:\n",
    "        str: The extracted English translation\n",
    "    \"\"\"\n",
    "    # Pattern to capture text after [en]: until next tag, note, or end\n",
    "    pattern = r'\\[en\\]:\\s*([\\s\\S]*?)(?=\\s*\\[[a-z]{2,}\\]:|\\.\\s*\\(|$)'\n",
    "    match = re.search(pattern, completion, re.IGNORECASE)\n",
    "    if match:\n",
    "        translation = match.group(1).strip()\n",
    "        return translation\n",
    "    else:\n",
    "        return completion.strip()\n",
    "\n",
    "    \n",
    "# test cases\n",
    "completion = \"do you know, one of the great pleasures of traveling and one of the joys of ethnographic research is to live together with people who can still remember the old days. those who still feel their past in the wind, who touch their past on rain-polished stones, who taste their past in the bitter leaves of plants. \\n[de]:wir haben uns in einem kleinen dorf in der savanne getroffen, wo die sonne ÃƒÂ¼ber den dÃƒÂ¤chern der hÃƒÂ¼tten schien, wo die vogel singen, wo die kÃƒÂ¶nige der savanne, die antilopen, ihre majestÃƒÂ¤t ÃƒÂ¼ber die weite savanne ausÃƒÂ¼ben. \\n[en]: we met in a small village in the savannah, where the sun shone over the roofs of the huts, where the birds sang, where the kings of the savannah, the antelopes,\"\n",
    "translation = extract_translation(completion)\n",
    "assert translation == \"we met in a small village in the savannah, where the sun shone over the roofs of the huts, where the birds sang, where the kings of the savannah, the antelopes,\"\n",
    "    \n",
    "completion = \"[en]: and of course we all share the same adaptation needs. (Note: The translation is not perfect, but itconveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation is not perfect, but it conveys the same meaning as the original text.) (Note: The translation\"\n",
    "translation = extract_translation(completion)\n",
    "assert translation == \"and of course we all share the same adaptation needs\"\n",
    "\n",
    "\n",
    "\n",
    "completion = \"[en]: [They] picked up these photographs from the forest floor, tried to see behind the face or the figure, found nothing and concluded that these were visiting cards of the devil and killed the five missionaries with the spear. \\n[en]: [They] picked up these photographs from the forest floor, tried to see behind the face or the figure, found nothing and concluded that these were visiting cards of the devil and killed the five missionaries with the spear. \\n[en]: [They] picked up these photographs from the forest floor, tried to see behind the face or the figure, found nothing and concluded that these were visiting cards of the devil and killed the five missionaries with the spear. \\n[en]: [They] picked up these photographs from the forest floor, tried to see behind the face or the figure, found nothing and concluded that these were visiting cards of the devil and killed the five missionaries with the spear. \\n[en]: [They] picked\"\n",
    "translation = extract_translation(completion)\n",
    "assert translation == \"[They] picked up these photographs from the forest floor, tried to see behind the face or the figure, found nothing and concluded that these were visiting cards of the devil and killed the five missionaries with the spear.\"\n",
    "\n",
    "completion = \"[en]:54 % of deaths were caused by perforations. \\n[en]: 54 % of deaths were caused by perforations. \\n[en]: 54 % of deaths were caused by perforations. \\n\"\n",
    "translation = extract_translation(completion)\n",
    "assert translation == \"54 % of deaths were caused by perforations.\"\n",
    "\n",
    "completion = \"no biologist would dare to claim that 50% or more of all species are on the brink of extinction, as it is simply not true. And yet, this â€“ the most apocalyptic scenario in the field of biological diversity â€“ is hardly the one we know as the most optimistic scenario in the field of cultural diversity. \\n[en]: no biologist would dare to claim that 50% or more of all species are on the brink of extinction, as it is simply not true. And yet, this â€“ the most apocalyptic scenario in the field of biological diversity â€“ is hardly the one we know as the most optimistic scenario in the field of cultural diversity. \\n[en]: no biologist would dare to claim that 50% or more of all species are on the brink of extinction, as it is simply not true. And yet, this â€“ the most apocalyptic scenario in the field of biological diversity â€“ is hardly the one we know as the most optimistic scenario in the field\"\n",
    "translation = extract_translation(completion)\n",
    "assert translation == \"no biologist would dare to claim that 50% or more of all species are on the brink of extinction, as it is simply not true. And yet, this â€“ the most apocalyptic scenario in the field of biological diversity â€“ is hardly the one we know as the most optimistic scenario in the field of cultural diversity.\"\n",
    "\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# run inference and save generations to a jsonl file\n",
    "\n",
    "prompt_template = open(\"prompts/mt/default.txt\").read()\n",
    "translations, references = [], []\n",
    "\n",
    "for item in tqdm(mt_test_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(**item) if prompt_template else item[\"source_text\"]\n",
    "    reference = item[\"target_text\"]\n",
    "    completion = llama.generate(prompt, \"mt\")  # Use task_type parameter\n",
    "    translation = extract_translation(completion)\n",
    "\n",
    "    translations.append(translation)\n",
    "    references.append(reference)\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    print(f\"translation: {translation}\")\n",
    "    print(f\"reference: {reference}\")\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.join(working_dir, \"generations\"), exist_ok=True)\n",
    "\n",
    "# Generate timestamp for the filename\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(working_dir, \"generations\", f\"mt_{timestamp}.jsonl\")\n",
    "\n",
    "# Save data\n",
    "with open(output_file, 'w') as f:\n",
    "    for i, (translation, reference, item) in enumerate(zip(translations, references, mt_test_data)):\n",
    "        data = {\n",
    "            \"id\": i,\n",
    "            \"source_text\": item[\"source_text\"],\n",
    "            \"translation\": translation,\n",
    "            \"reference\": reference\n",
    "        }\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "print(f\"Saved generations to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6659bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# run evaluations \n",
    "\n",
    "llama_mt_scores = defaultdict(list)\n",
    "\n",
    "for completion, reference in tqdm(zip(translations, references), desc=\"Evaluating dataset\"):\n",
    "    # compute sentence-level metrics\n",
    "    embedding_similarity = compute_embedding_similarity(completion, reference, labse)\n",
    "    llama_mt_scores[\"labse_embedding_similarity_list\"].append(embedding_similarity)\n",
    "\n",
    "    embedding_similarity = compute_embedding_similarity(completion, reference, gte)\n",
    "    llama_mt_scores[\"gte_embedding_similarity_list\"].append(embedding_similarity)\n",
    "\n",
    "# compute corpus-level scores properly\n",
    "corpus_bleu = compute_corpus_bleu(translations, references)\n",
    "corpus_ngram = compute_corpus_ngram_overlap(translations, references)\n",
    "\n",
    "llama_mt_scores[\"corpus_bleu\"] = corpus_bleu\n",
    "llama_mt_scores[\"corpus_ngram_overlap\"] = corpus_ngram\n",
    "llama_mt_scores[\"labse_embedding_similarity\"] = np.mean(llama_mt_scores[\"labse_embedding_similarity_list\"])\n",
    "llama_mt_scores[\"gte_embedding_similarity\"] = np.mean(llama_mt_scores[\"gte_embedding_similarity_list\"])\n",
    "\n",
    "for metric_name, metric_scores in llama_mt_scores.items():\n",
    "    print(f\"{metric_name}: {np.mean(metric_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479de065",
   "metadata": {},
   "source": [
    "In the evaluation results above, we computed embedding similarities using two different models: LaBSE and GTE.\n",
    "Analyze the results and explain:\n",
    "1. Is there any difference between the LaBSE and GTE similarity scores?\n",
    "2. Why might these differences occur?\n",
    "3. Which embedding model would you recommend for this MT evaluation task and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa62b74",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432a595",
   "metadata": {},
   "source": [
    "## 4. Short Story Generation Evaluation [13 points Programming + 4 points Non-Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e7224",
   "metadata": {},
   "source": [
    "In this section, we'll evaluate the quality of short stories generated by different LLMs.\n",
    "\n",
    "Unlike the previous tasks (multiple choice questions and machine translation), evaluating \n",
    "creative text generation is more subjective and challenging. There's no single \"correct\" \n",
    "output to compare against.\n",
    "\n",
    "We'll use several metrics to assess different aspects of the generated stories:\n",
    "\n",
    "1. **Perplexity**: Evaluates how \"surprising\" or predictable the text is\n",
    "2. **Distinct N-grams**: Measures lexical diversity and repetitiveness\n",
    "3. **Coherence**: Measures semantic flow between adjacent sentences using embedding similarity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532fe26",
   "metadata": {},
   "source": [
    "### 4.1. Temperature [0 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bd4f1",
   "metadata": {},
   "source": [
    "When generating text with language models, we can control the randomness of the output using a parameter called \"temperature\".\n",
    "Temperature controls how the model samples from the probability distribution over the vocabulary. \n",
    "#\n",
    "- **Low temperature** (e.g., 0.1-0.5): Makes the model more deterministic by amplifying high-likelihood tokens, producing predictable but potentially repetitive text. \n",
    "#\n",
    "- **High temperature** (e.g., 0.8-1.5): Flattens the probability distribution to increase randomness, giving lower-probability tokens more chances and producing more diverse, creative text that risks becoming incoherent if set too high.\n",
    "#\n",
    "In NLP, entropy refers to the unpredictability of text. High-entropy text is more diverse and unpredictable, while low-entropy text is more predictable and repetitive.\n",
    "#\n",
    "For story generation, finding the right temperature is crucial:\n",
    "- Too low: Stories become repetitive and boring\n",
    "- Too high: Stories may lose coherence and logical flow\n",
    "#\n",
    "You can change the temperature in your LLM by specifying it in the `LLMGenerationConfig` that you pass into your `LLM.generate()` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b80835",
   "metadata": {},
   "source": [
    "### 4.2. Perplexity Computation [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e558450",
   "metadata": {},
   "source": [
    "In this section, we'll implement a function to compute the perplexity of generated text.\n",
    "Perplexity is a common metric used to evaluate language models, measuring how \"surprised\" \n",
    "a model is by a given text. Lower perplexity indicates that the model finds the text more \n",
    "predictable and natural. \n",
    "\n",
    "Perplexity is defined as the exponentiated average negative log-likelihood of a sequence:\n",
    "\n",
    "$$P(W) = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log P(w_i|w_1,\\dots,w_{i-1})\\right)$$\n",
    "\n",
    "where W = (w_1, w_2, ..., w_N) is a sequence of N words, and P(w_i|w_1,...,w_{i-1}) is the\n",
    "conditional probability of word w_i given the preceding words w_1 through w_{i-1}.\n",
    "\n",
    "Note that there is no reason to use the same model for perplexity computation as the one used to generate text.\n",
    "In our experiments, we will use Phi-4, a model substantially larger than Llama used for generation.\n",
    "\n",
    "Implement the wrapper below for computing the perplexity using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def compute_perplexity(text: str, big_llm: LLM = None) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the perplexity of a given text using a language model.\n",
    "    \n",
    "    Perplexity is a measurement of how well a probability model predicts a sample.\n",
    "    Lower perplexity indicates the language model is better at predicting the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text to evaluate. This should be a coherent piece\n",
    "                   of text that we want to measure the perplexity of.\n",
    "        big_llm (LLM): A language model instance used for perplexity computation.\n",
    "                      If None is provided, a default LLM instance will be created.\n",
    "        \n",
    "    Returns:\n",
    "        float: The perplexity value of the input text. Lower values indicate the text\n",
    "              is more predictable according to the language model.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ### \n",
    "    pass\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da719839",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = LLM(hf_id=\"microsoft/phi-4\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - coherent text\n",
    "text = \"The transformer architecture uses multi-head self-attention mechanisms to process sequential data efficiently.\"\n",
    "perplexity = compute_perplexity(text, phi)\n",
    "assert torch.isclose(torch.tensor(perplexity, dtype=torch.float32), torch.tensor(8.227003675933073), atol=0.001)\n",
    "\n",
    "# Test - (seemingly) random nonsensical text (James Joyce - Finnegans Wake)\n",
    "text = \"Take. Bussoftlhee, mememormee! Till thousendsthee. Lps. The keys to. Given! A way a lone a last a loved a long the\"\n",
    "perplexity = compute_perplexity(text, phi)\n",
    "assert torch.isclose(torch.tensor(perplexity, dtype=torch.float32), torch.tensor(162.99996726850102), atol=0.001)\n",
    "\n",
    "# Test - archaic poetry (John Donne - A Valediction)\n",
    "text = \"If they be two, they are two so, as stiff twin compasses are two; Thy soul, the fixed foot, makes no show to move, but doth, if the other do.\"\n",
    "perplexity = compute_perplexity(text, phi)\n",
    "assert torch.isclose(torch.tensor(perplexity, dtype=torch.float32), torch.tensor(3.4699515280516278), atol=0.001)\n",
    "\n",
    "# Test - repetitive text\n",
    "text = \"The cat sat on the mat. The cat sat on the mat. The cat sat on the mat.\"\n",
    "perplexity = compute_perplexity(text, phi)\n",
    "assert torch.isclose(torch.tensor(perplexity, dtype=torch.float32), torch.tensor(4.433803583940793), atol=0.001)\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Fulh0MZ8y8b",
   "metadata": {},
   "source": [
    "### 4.3. N-gram Diversity [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e069a0",
   "metadata": {},
   "source": [
    "In this section, we'll implement a metric to evaluate the diversity of generated text.\n",
    "\n",
    "Distinct n-grams is a common metric used to measure the diversity and repetitiveness of generated text.\n",
    "It calculates the ratio of unique n-grams to the total number of n-grams in the text.\n",
    "\n",
    "A higher distinct n-gram ratio indicates:\n",
    "- More diverse vocabulary usage\n",
    "- Less repetition in the generated text\n",
    "- Potentially more creative and interesting content\n",
    "#\n",
    "This metric is particularly useful for evaluating story generation, where we want\n",
    "the model to produce varied and engaging content rather than repetitive patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def compute_ngram_diversity(text: str, n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the distinct n-gram ratio, which is a measure of text diversity.\n",
    "    \n",
    "    The distinct n-gram ratio is calculated by dividing the number of unique n-grams\n",
    "    by the total number of n-grams in the text. A higher ratio indicates more diverse text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The generated text to evaluate\n",
    "        n (int): The n-gram order (default: 2 for bigrams)\n",
    "        \n",
    "    Returns:\n",
    "        float: The ratio of unique n-grams to total n-grams, ranging from 0.0 to 1.0\n",
    "              where 1.0 means all n-grams are unique\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE ### \n",
    "    pass\n",
    "\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7138c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Test - all unique n-grams\n",
    "text = \"The quick brown fox jumps\"\n",
    "diversity = compute_ngram_diversity(text, n=2)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(1.0), atol=0.001)\n",
    "print(\"Test passed - all unique n-grams\")\n",
    "\n",
    "# Test - repetitive text (low diversity)\n",
    "text = \"the cat and the cat and the cat\"\n",
    "diversity = compute_ngram_diversity(text, n=2)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(0.42857142857), atol=0.001)\n",
    "print(\"Test passed - repetitive text\")\n",
    "\n",
    "# Test - single repeated n-gram (minimum diversity)\n",
    "text = \"ha ha ha ha ha\"\n",
    "diversity = compute_ngram_diversity(text, n=2)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(0.25), atol=0.001)\n",
    "print(\"Test passed - single repeated n-gram\")\n",
    "\n",
    "print(\"All tests passed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294cd98",
   "metadata": {},
   "source": [
    "### 4.4. Embedding Diversity [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff7a74",
   "metadata": {},
   "source": [
    "In this section, we'll implement a metric to evaluate the diversity of generated text using embeddings.\n",
    "\n",
    "While distinct n-grams measure lexical diversity (word-level), embedding diversity captures semantic diversity.\n",
    "This metric uses vector representations (embeddings) of the generated texts to measure how different they are\n",
    "from each other in the semantic space.\n",
    "\n",
    "The embedding diversity metric works by:\n",
    "1. Converting each generated text into an embedding vector\n",
    "2. Computing the cosine similarity between pairs of embeddings. In our implementation, we choose the first embedding to be the *reference* embedding, and compare all the other embeddings against this one. We do this for computational efficiency; a pairwise comparison would require $\\binom{N}{2} = O(N^2)$ comparisons whereas our approximations requires only $N-1 = O(N)$ comparisons. \n",
    "3. Calculating 1 minus the average similarity as the diversity score\n",
    "\n",
    "\n",
    "Implement this function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "def compute_embedding_diversity(\n",
    "    generations: List[str], \n",
    "    embedder: EmbeddingModel\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    This function computes the diversity of multiple generated texts using embeddings.\n",
    "    \n",
    "    The diversity score is calculated by embedding each generation, computing the cosine\n",
    "    similarity between the first generation (used as reference) and all other generations,\n",
    "    and then returning 1 minus the mean similarity. A higher score indicates more diverse\n",
    "    generations.\n",
    "    \n",
    "    Args:\n",
    "        generations (List[str]): A list of generated texts to evaluate for diversity\n",
    "        embedder (EmbeddingModel): The embedding model to use for converting text to vectors.\n",
    "                                  If None, a default EmbeddingModel will be instantiated.\n",
    "        \n",
    "    Returns:\n",
    "        float: Diversity score ranging from 0.0 to 1.0, where higher values indicate\n",
    "              more diverse generations. Returns 0.0 if fewer than 2 generations are provided.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ### \n",
    "    pass\n",
    "    ### END YOUR CODE ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ffebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Test - unique words\n",
    "text = [\"Bright crimson bird soars high\", \"Flying through azure skies freely\", \"Golden sunset paints the horizon\", \"Gentle waves caress sandy shores\", \"Ancient trees whisper forest secrets\"]\n",
    "diversity = compute_embedding_diversity(text, labse)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(0.6612603962421417), atol=0.001)\n",
    "print(\"Test passed - unique words\")\n",
    "# Test - repetitive text (low diversity)\n",
    "text = [\"hello world hello hello world\", \"hello world hello world hello\", \"hello hello world hello world\", \"world hello hello world hello\", \"hello world world hello hello\"]\n",
    "diversity = compute_embedding_diversity(text, labse)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(0.0066945552825927734), atol=0.001)\n",
    "print(\"Test passed - repetitive text\")\n",
    "\n",
    "# Test - single repeated word (minimum diversity)\n",
    "text = [\"aha aha aha aha aha aha aha\", \"aha aha aha aha aha aha\", \"aha aha aha aha aha\", \"aha aha aha aha\", \"aha aha aha\"]\n",
    "diversity = compute_embedding_diversity(text, labse)\n",
    "assert torch.isclose(torch.tensor(diversity, dtype=torch.float32), torch.tensor(0.03923851251602173), atol=0.001)\n",
    "print(\"Test passed - single repeated word\")\n",
    "\n",
    "print(\"All tests passed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TQVT6HUA9htQ",
   "metadata": {},
   "source": [
    "### 4.5. Coherence Metric [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba883cc",
   "metadata": {},
   "source": [
    "Next, we explore a custom coherence metric. We would like to measure how well the generated text maintains logical flow and semantic relatedness between adjacent sentences.\n",
    "#\n",
    "In this implementation, we use embedding-based semantic similarity to quantify coherence. \n",
    "The function computes the score by: \n",
    "\n",
    "1. Splitting each generated text into sentences\n",
    "2. Computing embeddings for each sentence using the provided embedding model\n",
    "3. Calculating cosine similarity between adjacent sentence pairs\n",
    "4. Averaging these similarities to produce a coherence score for each text\n",
    "5. Returning the mean coherence score for the given list of sentences. \n",
    "\n",
    "\n",
    "\n",
    "Higher coherence scores (closer to 1.0) indicate better logical flow between sentences,\n",
    "while lower scores (closer to 0.0) suggest disconnected or inconsistent text.\n",
    "\n",
    "Implement the `compute_coherence` function below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74501f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Split text at punctuation marks: ., !, ?\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to split into sentences\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of sentences with whitespace stripped\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(f\"Expected string input, got {type(text)}\")\n",
    "    \n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def compute_coherence(sentences: List[str], model: EmbeddingModel) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the coherence of a text based on the semantic similarity\n",
    "    between adjacent sentences.\n",
    "    \n",
    "    Coherence measures how well the sentences in a text connect to each other in a logical\n",
    "    and consistent way. Higher coherence indicates a more natural flow between sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences (List[str]): A list of sentences to evaluate for coherence\n",
    "        model (EmbeddingModel): The embedding model to use for computing text embeddings\n",
    "        \n",
    "    Returns:\n",
    "        float: Coherence score. Higher values indicate better coherence between \n",
    "               adjacent sentences. Returns 0.0 if fewer than 2 sentences are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    #### YOUR CODE HERE ####\n",
    "    \n",
    "    pass\n",
    "\n",
    "    #### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - single sentence\n",
    "generations = [\"Only one sentence here\"]\n",
    "coherence = compute_coherence(generations, labse)\n",
    "\n",
    "assert torch.isclose(\n",
    "    torch.tensor(coherence, dtype=torch.float32),\n",
    "    torch.tensor(0.0),\n",
    "    atol=0.001\n",
    ")\n",
    "print(\"Test passed - single sentence\")\n",
    "\n",
    "# Test - repeated sentences\n",
    "generations = [\"Bright crimson bird soars high.\",\n",
    "                \"Bright crimson bird soars high.\",\n",
    "                \"Bright crimson bird soars high.\"]\n",
    "coherence = compute_coherence(generations, labse)\n",
    "\n",
    "\n",
    "assert torch.isclose(\n",
    "    torch.tensor(coherence, dtype=torch.float32),\n",
    "    torch.tensor(1.0),\n",
    "    atol=0.001\n",
    ")\n",
    "print(\"Test passed - identical sentences\")\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zeo9kX6i9pbH",
   "metadata": {},
   "source": [
    "### 4.6. Ensemble Evaluation Pipeline [4 points Non-Programming]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177dbe0e",
   "metadata": {},
   "source": [
    "Now let's evaluate our short story generation metrics using the ROC Stories dataset on the Llama model.\n",
    "\n",
    "The ROC Stories dataset contains five-sentence stories that capture causal and temporal commonsense relations between everyday events. For each sample in the dataset, we take the first two sentences and have the model generate the remaining three sentences. \n",
    "\n",
    "Unlike MCQ and MT tasks where we compare against a single correct answer, story generation is open-ended. Therefore, we'll generate **multiple outputs per prompt** (15 generations per story) to analyze the distribution of coherence scores across different temperature settings.\n",
    "\n",
    "In the following cells, we evaluate generations using the default temperature (0.7) from the LLM configuration. For a complete analysis, you would want to experiment with different temperatures (0.5, 1.0, 1.5) and analyze how temperature affects narrative quality, coherence, and diversity of the generated stories.\n",
    "\n",
    "Run the following cells. For each evaluation result, comment on the final values. What does this say about the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Load the ROC Stories data for continuation task\n",
    "roc_continuation_data_raw = pd.read_csv(\"data/ssg/roc_completion.csv\")\n",
    "print(f\"Loaded {len(roc_continuation_data_raw)} stories for the continuation task\")\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "roc_continuation_data_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# run inference using Llama and save generations to a jsonl file for continuation task\n",
    "\n",
    "prompt_template = open(\"prompts/ssg/default.txt\").read()\n",
    "all_completions = []\n",
    "references = []\n",
    "num_generations = 15\n",
    "\n",
    "# Create a list to store continuation data\n",
    "roc_continuation_data = []\n",
    "\n",
    "items = list(roc_continuation_data_raw.iterrows())\n",
    "\n",
    "# Load the ROC Stories data for continuation task\n",
    "for _, item in tqdm(items, desc=\"Evaluating dataset\"):\n",
    "    prompt = prompt_template.format(sentence=item['first_two_sentences'])\n",
    "    first_two_sentences = item['first_two_sentences']\n",
    "\n",
    "    # For story continuation, we don't have a reference in the dataset\n",
    "    # We're generating completions to stories that start with the first two sentences\n",
    "\n",
    "    completions = []\n",
    "    for _ in tqdm(range(num_generations), desc=\"Generating completions\"):\n",
    "        completion = llama.generate(prompt, \"ssg\")  # Use ssg task_type for story generation\n",
    "        completions.append(completion)\n",
    "\n",
    "    # Create continuation data item with consistent structure\n",
    "    eval_item = {\n",
    "        \"storyid\": item[\"storyid\"],\n",
    "        \"prompt\": first_two_sentences,\n",
    "        \"completions\": completions\n",
    "    }\n",
    "    roc_continuation_data.append(eval_item)\n",
    "\n",
    "    all_completions.append(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.join(working_dir, \"generations\"), exist_ok=True)\n",
    "\n",
    "# Generate timestamp for the filename\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(working_dir, f\"generations/ssg_continuation_{timestamp}.jsonl\")\n",
    "\n",
    "# Save data\n",
    "with open(output_file, 'w') as f:\n",
    "    for i, item in enumerate(roc_continuation_data):\n",
    "        data = {\n",
    "            \"storyid\": item[\"storyid\"],\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"completions\": item[\"completions\"]\n",
    "        }\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "print(f\"Saved generations to {output_file}\")\n",
    "print(f\"Created {len(roc_continuation_data)} items for continuation evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f9cf4",
   "metadata": {},
   "source": [
    "Next, we will run evaluations on the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# run evaluations for coherence\n",
    "llama_continuation_scores = defaultdict(list)\n",
    "\n",
    "# Evaluate coherence for each story\n",
    "for item in tqdm(roc_continuation_data, desc=\"Evaluating dataset\"):\n",
    "    prompt = item[\"prompt\"]\n",
    "    for completion in tqdm(item[\"completions\"], desc=\"Evaluating completions\"):\n",
    "        # Split the prompt and completion into sentences\n",
    "        prompt_sentences = split_sentences(prompt)\n",
    "        completion_sentences = split_sentences(completion)\n",
    "        \n",
    "        # Combine prompt sentences and completion sentences for coherence evaluation\n",
    "        all_sentences = prompt_sentences + completion_sentences\n",
    "        \n",
    "        # Calculate coherence between all sentences\n",
    "        if len(all_sentences) > 1:  # Need at least 2 sentences for coherence\n",
    "            coherence_score = compute_coherence(all_sentences, gte)\n",
    "            llama_continuation_scores[\"coherence_list\"].append(coherence_score)\n",
    "    \n",
    "# Compute average scores\n",
    "if llama_continuation_scores[\"coherence_list\"]:\n",
    "    llama_continuation_scores[\"coherence\"] = np.mean(llama_continuation_scores[\"coherence_list\"])\n",
    "    llama_continuation_scores[\"coherence_std\"] = np.std(llama_continuation_scores[\"coherence_list\"])\n",
    "\n",
    "# Print results\n",
    "for metric_name, metric_value in llama_continuation_scores.items():\n",
    "    if not metric_name.endswith(\"_list\"):\n",
    "        print(f\"{metric_name}: {metric_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# compute perplexity for each completion\n",
    "llama_ssg_scores = defaultdict(list)\n",
    "\n",
    "# Compute perplexity for each completion\n",
    "for completions in tqdm(all_completions, desc=\"Computing perplexity\"):\n",
    "    # Calculate perplexity using the language model\n",
    "    perplexity_list = [compute_perplexity(completion, llama) for completion in completions]\n",
    "    llama_ssg_scores[\"perplexity_list\"].append(perplexity_list)\n",
    "    \n",
    "# Compute average perplexity\n",
    "llama_ssg_scores[\"perplexity_mean\"] = np.mean(llama_ssg_scores[\"perplexity_list\"])\n",
    "llama_ssg_scores[\"perplexity_std\"] = np.std(llama_ssg_scores[\"perplexity_list\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Perplexity: {llama_ssg_scores['perplexity_mean']:.3f} Â± {llama_ssg_scores['perplexity_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# compute n-gram diversity for each completion\n",
    "# Note: llama_ssg_scores was already initialized in previous cell\n",
    "\n",
    "# Compute n-gram diversity for each completion\n",
    "for completions in tqdm(all_completions, desc=\"Computing n-gram diversity\"):\n",
    "    # Calculate n-gram diversity using the language model\n",
    "    for n in [2, 3, 4]:\n",
    "        ngram_diversity_list = [compute_ngram_diversity(completion, n) for completion in completions]\n",
    "        llama_ssg_scores[f\"{n}_gram_diversity_list\"].append(ngram_diversity_list)\n",
    "    \n",
    "# Compute average n-gram diversity\n",
    "for n in [2, 3, 4]:\n",
    "    llama_ssg_scores[f\"{n}_gram_diversity_mean\"] = np.mean(llama_ssg_scores[f\"{n}_gram_diversity_list\"])\n",
    "    llama_ssg_scores[f\"{n}_gram_diversity_std\"] = np.std(llama_ssg_scores[f\"{n}_gram_diversity_list\"])\n",
    "\n",
    "# Print results\n",
    "for n in [2, 3, 4]:\n",
    "    print(f\"{n}-gram diversity: {llama_ssg_scores[f'{n}_gram_diversity_mean']:.3f} Â± {llama_ssg_scores[f'{n}_gram_diversity_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# compute embedding diversity using LaBSE for each completion\n",
    "# Note: llama_ssg_scores was already initialized in previous cell\n",
    "\n",
    "# Compute embedding diversity for each completion\n",
    "for completions in tqdm(all_completions, desc=\"Computing embedding diversity using LaBSE\"):\n",
    "    # Calculate embedding diversity using the embedding model\n",
    "    embedding_diversity = compute_embedding_diversity(completions, labse)\n",
    "    llama_ssg_scores[\"embedding_labse_diversity_list\"].append(embedding_diversity)\n",
    "    \n",
    "# Compute average embedding diversity\n",
    "llama_ssg_scores[\"embedding_labse_diversity_mean\"] = np.mean(llama_ssg_scores[\"embedding_labse_diversity_list\"])\n",
    "llama_ssg_scores[\"embedding_labse_diversity_std\"] = np.std(llama_ssg_scores[\"embedding_labse_diversity_list\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Embedding diversity using LaBSE: {llama_ssg_scores['embedding_labse_diversity_mean']:.3f} Â± {llama_ssg_scores['embedding_labse_diversity_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df99858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "\n",
    "# compute embedding diversity using GTE for each completion\n",
    "# Note: llama_ssg_scores was already initialized in previous cell\n",
    "\n",
    "# Compute embedding diversity for each completion\n",
    "for completions in tqdm(all_completions, desc=\"Computing embedding diversity using GTE\"):\n",
    "    # Calculate embedding diversity using the embedding model\n",
    "    embedding_diversity = compute_embedding_diversity(completions, gte)\n",
    "    llama_ssg_scores[\"embedding_gte_diversity_list\"].append(embedding_diversity)\n",
    "    \n",
    "# Compute average embedding diversity\n",
    "llama_ssg_scores[\"embedding_gte_diversity_mean\"] = np.mean(llama_ssg_scores[\"embedding_gte_diversity_list\"])\n",
    "llama_ssg_scores[\"embedding_gte_diversity_std\"] = np.std(llama_ssg_scores[\"embedding_gte_diversity_list\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Embedding diversity using GTE: {llama_ssg_scores['embedding_gte_diversity_mean']:.3f} Â± {llama_ssg_scores['embedding_gte_diversity_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467bbf8",
   "metadata": {},
   "source": [
    "## 5. Sampling Hyperparameters and Prompt Optimization [BONUS - 4 points]\n",
    "\n",
    "In this section, we explore how different choices of prompt engineering and regex patterns can affect the accuracy on MMLU.\n",
    "\n",
    "Your task is to:\n",
    "1. Experiment with various prompting techniques (few-shot prompting, chain-of-thought, etc.) to maximize performance\n",
    "2. Optimize the regex patterns to improve answer extraction accuracy\n",
    "3. Document the impact of different hyperparameters (temperature, top_p, top_k) on model performance\n",
    "\n",
    "Compare your results with the baseline approach and explain which techniques were most effective and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db6978",
   "metadata": {},
   "source": [
    "## 6. Submitting Your Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e93176",
   "metadata": {},
   "source": [
    "This is the end. Congratulations!  \n",
    "\n",
    "Now, follow the steps below to submit your homework on Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47daf5",
   "metadata": {},
   "source": [
    "### 6.1. Programming\n",
    "\n",
    "The programming will be evaluated through an autograder. To create the file to submit for autograder, follow the steps below -\n",
    "1. Open a terminal from the root directory of the project\n",
    "2. Run the collect_submission.py file\n",
    "3. Agree to the Late Policy and Honor Pledge\n",
    "4. After the file is executed, your root project will have a submission directory.\n",
    "5. Submit all the contents of this file to GradeScope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13221269",
   "metadata": {},
   "source": [
    "### 6.2. Non-Programming\n",
    "\n",
    "The analysis parts will be evaluated manually. For this, export the notebook to a PDF file, and submit it on GradeScope. Please ensure no written code or output is clipped when you create your PDF. One reliable way to do it is first download it as HTML through Jupyter Notebook and then print it to get PDF."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hopefully_working_env",
   "language": "python",
   "name": "nlp-hw1-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
