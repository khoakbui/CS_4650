
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍
#################################################
# file to edit: CS4650_hw1_release_fa2025.ipynb͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍


# DO NOT CHANGE THIS CELL
# Importing required libraries - DO NOT CHANGE THIS CELL

import os
import sys
import json
import pandas as pd
import datetime
from collections import Counter, defaultdict
import re
from dataclasses import dataclass
from typing import Callable, Dict, List, Sequence, Tuple, Optional
from tqdm import tqdm
import random
import numpy as np

import torch
import torch.nn.functional as F

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer


# DO NOT CHANGE THIS CELL

# Import tokenizer for n-gram matching

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')


# DO NOT CHANGE THIS CELL
# Defining global constants - DO NOT CHANGE THESE VALUES

RANDOM_SEED = 42
PADDING_VALUE = 0
UNK_VALUE     = 1
BATCH_SIZE = 128

torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

working_dir = os.getcwd()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# DO NOT CHANGE THIS CELL

@dataclass
class LLMGenerationConfig:
    """
    Configuration class for LLM generation parameters.
    This is for convenience for keeping track of the generation parameters.
    http://brainiac:9657/tree?token=9d0d4afb1bc218c5d85f6418e7d1e8007ef0dc1b2a44bd28
    Args:
        temperature (float): Controls randomness in sampling. Higher values make output more random.
        max_new_tokens (int): Maximum number of new tokens to generate.
    """
    temperature: float = 0.7
    max_new_tokens: int = 100


class LLM:
    """
    A wrapper class for Hugging Face language models that provides a unified interface
    for text generation, logit computation, and perplexity calculation.

    If transformers library is not available, falls back to deterministic stubs.
    """

    def __init__(self, hf_id: str = "gpt2", device: str = None, quantize: bool = True):
        """
        Initialize the LLM wrapper.

        Args:
            hf_id (str): Hugging Face model identifier
            device (str): Device to load model on ('cuda', 'cpu', 'mps')
        """
        self.hf_id = hf_id
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Use auto-loading with device_map="auto" for faster loading and automatic memory management
        self.tokenizer = AutoTokenizer.from_pretrained(hf_id)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        if device == 'cpu':
            self.model = AutoModelForCausalLM.from_pretrained(
                hf_id,
                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory
                load_in_8bit=False
            ).to(self.device).eval()
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                hf_id,
                device_map="auto",  # Automatically determine optimal device mapping
                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory
                load_in_8bit=quantize,  # Enable 4-bit quantization for even more memory efficiency
            ).eval()


    @torch.inference_mode()
    def generate(self, prompt: str, task_type: str = "mcq", config: LLMGenerationConfig = None) -> str:
        """
        Generate text continuation for the given prompt using the underlying language model.

        This method takes a text prompt and generates additional text that continues from
        the prompt in a coherent manner. The generation process can be controlled through
        various parameters specified in the config object.

        The method tokenizes the input prompt, passes it through the model, and then
        decodes the generated token IDs back to text, excluding the original prompt tokens.

        Args:
            prompt (str): Input text prompt that the model will continue from
            config (LLMGenerationConfig): Configuration object containing generation parameters
                such as temperature, top_p, top_k, and max_new_tokens. If None, default
                parameters will be used.

        Returns:
            str: Generated text continuation without the original prompt. The text is
                stripped of leading/trailing whitespace and special tokens are removed
                during decoding.
        """

        # set generation config
        assert task_type in ["mcq", "mt", "ssg"], "Invalid task_type. Must be one of: mcq, mt, ssg"

        if config is not None:
            # Use the provided config
            pass
        elif task_type == "mcq":
            config = LLMGenerationConfig(
                temperature=0.0,
                max_new_tokens=4,
            )
        elif task_type == "mt":
            config = LLMGenerationConfig(
                temperature=0.0,
                max_new_tokens=200, # max llama=189, qwen=188 on test set; mt contains longer sentences
            )
        elif task_type == "ssg":
            config = LLMGenerationConfig(
                temperature=0.7,
                max_new_tokens=100, # max llama=76, qwen=75 on all five sentences
                )



        # tokenize the prompt
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)

        # generate the output
        if config.temperature > 0:
            output = self.model.generate(
                input_ids,
                do_sample=True,
                temperature=config.temperature,
                max_length=input_ids.shape[1] + config.max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        else:
            output = self.model.generate(
                input_ids,
                do_sample=False,
                max_length=input_ids.shape[1] + config.max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
            )

        generated = self.tokenizer.decode(
            output[0, input_ids.shape[1]:],
            skip_special_tokens=True
        )
        return generated.strip()

    @torch.inference_mode()
    def logits(self, prompt: str) -> torch.Tensor:
        """
        Get next-token logits for the given prompt.

        This method computes and returns the logits (raw, unnormalized prediction scores)
        for the next token that would follow the given prompt. These logits represent the model's
        prediction distribution over the entire vocabulary for the next token position.

        Args:
            prompt (str): Input text prompt for which to compute next-token predictions

        Returns:
            torch.Tensor: A tensor of shape (vocab_size,) containing the logits for each
                possible next token in the vocabulary. Higher values indicate tokens the
                model considers more likely to follow the prompt.
        """
        tokens = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model(**tokens)
        # Return logits for the last token position


        #### YOUR CODE HERE ####

        pass

        #### END YOUR CODE ####



    ### DO NOT CHANGE THIS FUNCTION ###
    @torch.inference_mode()
    def perplexity(self, text: str) -> float:
        """
        Calculate perplexity of the given text.

        Perplexity is a measurement of how well a probability model predicts a sample.
        Lower perplexity indicates the model is better at predicting the text.
        It is calculated as the exponentiated average negative log-likelihood of a sequence.

        Args:
            text (str): Input text for which to calculate perplexity
        Returns:
            float: Perplexity value
        """
        if self.model is None:
            return 100.0  # Fixed stub value

        encodings = self.tokenizer(text, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(**encodings)
            logits = outputs.logits

        # Shift logits and labels for next-token prediction
        shift_logits = logits[:, :-1].contiguous()
        shift_labels = encodings.input_ids[:, 1:].contiguous()

        # Calculate cross-entropy loss
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)
        loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1)
        )

        return np.exp(loss.item())


# DO NOT CHANGE THIS CELL

class EmbeddingModel:
    """
    A wrapper around the Huggingface SentenceTransformer API for generating embeddings.
    This model creates semantic embeddings that can be used for measuring similarity
    between texts.
    """

    def __init__(self, hf_id: str = "sentence-transformers/all-MiniLM-L6-v2", dim: int = None):
        """
        Initialize the embedding model with the specified model ID.

        Args:
            hf_id (str): Hugging Face model ID for the SentenceTransformer model.
                         Default is "sentence-transformers/all-MiniLM-L6-v2".
            dim (int): Not used for SentenceTransformer models as the dimension is
                       determined by the model itself, but kept for API compatibility.
        """
        self.model = SentenceTransformer(hf_id,  trust_remote_code=True)
        self.dim = self.model.get_sentence_embedding_dimension()

    def embed(self, text: str) -> torch.Tensor:
        """
        Create an embedding for the given text using the SentenceTransformer model.

        Args:
            text (str): Input text to embed. Can be of any length.

        Returns:
            torch.Tensor: Embedding vector representing the semantic content of the input text.
        """
        # SentenceTransformer returns numpy array, convert to torch tensor
        embedding = self.model.encode(text, convert_to_tensor=True)
        return embedding


### YOUR CODE HERE ###

_mcq_regex = None

### END YOUR CODE ###

def compute_regex_accuracy(generation: str, reference: str) -> tuple[float, str]:
    """
    Extract the first letter A-D from the generation and compare with reference.

    Args:
        generation (str): Generated text from the model
        reference (str): Ground truth answer (A, B, C, or D)

    Returns:
        float: 1.0 if correct, 0.0 if incorrect
        str: The predicted letter (A, B, C, or D)
    """
    match = _mcq_regex.search(generation)
    regex_pred = match.group(1).upper() if match else None
    return float(regex_pred == reference.strip().upper()), regex_pred

LETTERS = ["A", "B", "C", "D"]

def compute_logit_accuracy(logits: torch.Tensor, reference: str, model: LLM, valid_letters: List[str] = None) -> Tuple[float, str]:
    """
    Compute accuracy based on logits for the first token generated by the LLM.

    Args:
        logits (torch.Tensor): Logits from the model for the first token
        reference (str): Ground truth answer (A, B, C, or D)
        model (LLM): The language model used for generation, needed for tokenizer access
        valid_letters (List[str]): List of valid letters to restrict argmax to (e.g., ['A', 'B', 'C', 'D'])
                                If None, uses the entire vocabulary

    Returns:
        float: 1.0 if the token with highest logit matches reference, 0.0 otherwise
        str: The predicted letter (If `valid_letters` is None, then the predicted letter is not constrained to be one of the valid letters, but can be any letter in the vocabulary)
    """

    ### YOUR CODE HERE ###

    if valid_letters is not None:
        # limit vocabulary to only the valid letters
        pass
    else:
        # use entire vocabulary
        pass

    # compute accuracy
    pass

    ### END YOUR CODE ###


# DO NOT CHANGE THIS CELL

def preprocess_text(text: str) -> List[str]:
    """
    Tokenize a text string into a list of tokens. This is only for n-gram matching.

    Args:
        text (str): The input text string to tokenize

    Returns:
        List[str]: A list of tokens extracted from the input text
    """
    return nltk.word_tokenize(text)


def _ngrams(tokens: Sequence[str], n: int) -> List[Tuple[str, ...]]:
    """
    Extract n-grams from a sequence of tokens.

    Args:
        tokens (Sequence[str]): A sequence of tokens (words, characters, etc.)
        n (int): The size of each n-gram

    Returns:
        List[Tuple[str, ...]]: A list of tuples, where each tuple contains n consecutive tokens
                              from the input sequence
    """
    ### YOUR CODE HERE ###
    pass
    ### END YOUR CODE ###


def compute_corpus_ngram_overlap(candidates: List[str], references: List[str], n: int = 2) -> float:
    """
    Compute corpus-level n-gram overlap by aggregating across all sentences.
    Compute the n-gram overlap, accounting for the number of times each n-gram appears in the candidate and reference.

    Args:
        candidates (List[str]): List of candidate translations
        references (List[str]): List of reference translations
        n (int): N-gram order

    Returns:
        float: Corpus-level n-gram overlap score
    """
    ### YOUR CODE HERE ###
    # Initialize counters for n-grams
    pass

    # Calculate overlap considering frequencies
    pass

    ### END YOUR CODE ###


def compute_corpus_bleu(candidates: List[str], references: List[str], max_n: int = 4) -> float:
    """
    Compute corpus-level BLEU score by aggregating n-gram statistics across all sentences.

    Args:
        candidates (List[str]): List of candidate translations
        references (List[str]): List of reference translations
        max_n (int): Maximum n-gram order to consider

    Returns:
        float: Corpus-level BLEU score
    """
    # Aggregate statistics across all sentences
    total_cand_len = 0
    total_ref_len = 0
    ngram_matches = [0] * max_n
    ngram_totals = [0] * max_n

    # iterate over each candidate and reference
    for cand, ref in zip(candidates, references):
        cand_tokens = preprocess_text(cand)
        ref_tokens = preprocess_text(ref)

        total_cand_len += len(cand_tokens)
        total_ref_len += len(ref_tokens)

        for n in range(1, max_n + 1):
            cand_ngrams = Counter(_ngrams(cand_tokens, n))
            ref_ngrams = Counter(_ngrams(ref_tokens, n))

            overlap = {ng: min(count, ref_ngrams[ng]) for ng, count in cand_ngrams.items()}
            ngram_matches[n-1] += sum(overlap.values())
            ngram_totals[n-1] += sum(cand_ngrams.values())

    precisions = []

    ### YOUR CODE HERE ###

    # calculate precision for each n-gram order
    pass

    ### END YOUR CODE ###

    # Geometric mean with smoothing
    eps = 1e-9
    geo_mean = np.exp(sum(np.log(p + eps) for p in precisions) / max_n)

    # Brevity penalty
    bp = 1.0 if total_cand_len > total_ref_len else np.exp(1 - total_ref_len / max(total_cand_len, 1))

    return bp * geo_mean


def compute_embedding_similarity(
    generation: str,
    reference: str,
    embedder: EmbeddingModel = None
) -> float:
    """
    This function computes the semantic similarity between a generated text and a reference text
    using embeddings and cosine similarity.

    Embedding-based similarity captures semantic meaning beyond exact word matches, allowing
    for evaluation of paraphrases and texts that convey similar meaning with different words.

    Args:
        generation (str): Generated text to be evaluated
        reference (str): Reference text to compare against
        embedder (EmbeddingModel): Model to create text embeddings. If None, a default model is used.

    Returns:
        float: Cosine similarity between the embeddings, ranging from -1.0 to 1.0,
              where higher values indicate greater semantic similarity
    """
    ### YOUR CODE HERE ###

    pass

    ### END YOUR CODE ###


def compute_perplexity(text: str, big_llm: LLM = None) -> float:
    """
    This function computes the perplexity of a given text using a language model.

    Perplexity is a measurement of how well a probability model predicts a sample.
    Lower perplexity indicates the language model is better at predicting the text.

    Args:
        text (str): The generated text to evaluate. This should be a coherent piece
                   of text that we want to measure the perplexity of.
        big_llm (LLM): A language model instance used for perplexity computation.
                      If None is provided, a default LLM instance will be created.

    Returns:
        float: The perplexity value of the input text. Lower values indicate the text
              is more predictable according to the language model.
    """
    ### YOUR CODE HERE ###
    pass
    ### END YOUR CODE ###


def compute_ngram_diversity(text: str, n: int = 2) -> float:
    """
    This function computes the distinct n-gram ratio, which is a measure of text diversity.

    The distinct n-gram ratio is calculated by dividing the number of unique n-grams
    by the total number of n-grams in the text. A higher ratio indicates more diverse text.

    Args:
        text (str): The generated text to evaluate
        n (int): The n-gram order (default: 2 for bigrams)

    Returns:
        float: The ratio of unique n-grams to total n-grams, ranging from 0.0 to 1.0
              where 1.0 means all n-grams are unique
    """

    ### YOUR CODE HERE ###
    pass

    ### END YOUR CODE ###

# DO NOT CHANGE THIS CELL

def compute_embedding_diversity(
    generations: List[str],
    embedder: EmbeddingModel
) -> float:
    """
    This function computes the diversity of multiple generated texts using embeddings.

    The diversity score is calculated by embedding each generation, computing the cosine
    similarity between the first generation (used as reference) and all other generations,
    and then returning 1 minus the mean similarity. A higher score indicates more diverse
    generations.

    Args:
        generations (List[str]): A list of generated texts to evaluate for diversity
        embedder (EmbeddingModel): The embedding model to use for converting text to vectors.
                                  If None, a default EmbeddingModel will be instantiated.

    Returns:
        float: Diversity score ranging from 0.0 to 1.0, where higher values indicate
              more diverse generations. Returns 0.0 if fewer than 2 generations are provided.
    """
    ### YOUR CODE HERE ###
    pass
    ### END YOUR CODE ###


def split_sentences(text):
    """
    Split text at punctuation marks: ., !, ?

    Args:
        text (str): Input text to split into sentences

    Returns:
        List[str]: List of sentences with whitespace stripped
    """
    if not isinstance(text, str):
        raise ValueError(f"Expected string input, got {type(text)}")

    sentences = re.split(r'[.!?]', text)
    return [s.strip() for s in sentences if s.strip()]


def compute_coherence(sentences: List[str], model: EmbeddingModel) -> float:
    """
    This function computes the coherence of a text based on the semantic similarity
    between adjacent sentences.

    Coherence measures how well the sentences in a text connect to each other in a logical
    and consistent way. Higher coherence indicates a more natural flow between sentences.

    Args:
        sentences (List[str]): A list of sentences to evaluate for coherence
        model (EmbeddingModel): The embedding model to use for computing text embeddings

    Returns:
        float: Coherence score. Higher values indicate better coherence between
               adjacent sentences. Returns 0.0 if fewer than 2 sentences are provided.
    """

    #### YOUR CODE HERE ####

    pass

    #### END YOUR CODE ####