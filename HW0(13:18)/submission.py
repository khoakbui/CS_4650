
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍
#################################################
# file to edit: CS4650_HW0_Fall2025.ipynb͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍

import os
import sys
import numpy as np
import matplotlib.pyplot as plt

EPSILON = 1e-8 # A small value to be used in section 3.3.


def sigmoid(z):

    z = np.asarray(z, dtype=np.float64)   # ensure vectorized
    return 1.0 / (1.0 + np.exp(-z))



def predict_proba(theta, X):
    '''
    Computes the predicted probabilities for a logistic regression model.

    Parameters:
    -----------
    theta : numpy.ndarray
        The model parameters of shape (1, n_features + 1). These parameters are used to
        compute the linear combination of the features.

    x : numpy.ndarray
        The input feature matrix of shape (n_samples, n_features + 1). Each row corresponds
        to a sample, and each column corresponds to a feature.

    Returns:
    --------
    h : numpy.ndarray
        A vector of shape (n_samples,) containing the predicted probabilities for each
        sample. Each value in the vector represents the probability of the corresponding
        sample belonging to the positive class (label 1).
    '''
    # Ensure arrays
    X = np.asarray(X, dtype=np.float64)
    Theta = np.asarray(theta, dtype=np.float64)

    # Normalize theta to 2D:
    # - (n_features,) -> (n_features, 1)
    # - (n_models, n_features) -> transpose to (n_features, n_models)
    if Theta.ndim == 1:
        Theta = Theta.reshape(-1, 1)
    elif Theta.shape[0] != X.shape[1] and Theta.shape[1] == X.shape[1]:
        Theta = Theta.T  # make it (n_features, n_models)

    # Now: X (n_samples, n_features) @ Theta (n_features, n_models)
    z = X @ Theta
    h = sigmoid(z)  # shape: (n_samples, n_models); (n_samples,1) if single model
    return h



def log_likelihood_positive(h, y):
    """
    Computes the log likelihood of the positive examples given the data.

    Parameters
    ----------
    h : numpy.ndarray
        A numpy array of shape (n_samples, 1) containing the predicted probabilities
        generated from predict_proba.

    y : numpy.ndarray
        A numpy array of shape (n_samples,) containing the true labels for the data.
        The labels are 1 for positive examples and 0 for negative examples.

    Returns
    -------
    l_p : numpy.ndarray
        A numpy array of shape (1,) containing a single value representing the log likelihood
        of the positive examples given the data.

        Note, we will not divide by n here, that will be done in Section 3.3.3.
    """
    epsilon = 1e-8
    h = np.asarray(h).reshape(-1)
    y = np.asarray(y).reshape(-1)
    l_p = np.sum(np.log(h[y == 1] + epsilon))
    return np.array([l_p], dtype=float)



def log_likelihood_negative(h, y):
    """
    Computes the log likelihood of the negative examples given the data.

    Parameters
    ----------
    h : numpy.ndarray
        A numpy array of shape (n_samples, 1) containing the predicted probabilities
        generated from predict_proba.

    y : numpy.ndarray
        A numpy array of shape (n_samples,) containing the true labels for the data.
        The labels are 1 for positive examples and 0 for negative examples.

    Returns
    -------
    l_n : numpy.ndarray
        A numpy array of shape (1,) containing a single value representing the log likelihood
        of the negative examples given the data.

        Note, we will not divide by n here, that will be done in Section 3.3.3.
    """
    epsilon = 1e-8                 # avoid log(0)
    h = np.asarray(h).reshape(-1)  # flatten to (n,)
    y = np.asarray(y).reshape(-1)  # flatten to (n,)
    l_n = np.sum(np.log(1.0 - h[y == 0] + epsilon))
    return np.array([l_n], dtype=float)



def discriminative_negative_log_likelihood(h, y, theta=None, lambda_=0.0):
    """
    Computes the regularized negative log likelihood of the labels y given the data.

    Parameters
    ----------
    h : numpy.ndarray
        A numpy array of shape (n_samples, 1) containing the predicted probabilities
        generated from predict_proba.

    y : numpy.ndarray
        A numpy array of shape (n_samples,) containing the true labels for the data.
        The labels are 1 for positive examples and 0 for negative examples.

    theta : numpy.ndarray, optional
        A numpy array of shape (1, n_features + 1) containing the model parameters. This is
        required if L2 regularization is applied. 0th dimension contains the bias term's coefficient.

    lambda_ : float, optional
        The regularization strength (λ). This value is multiplied by the L2 penalty term
        to control the impact of regularization. Default is 0.0 (no regularization).

    Returns
    -------
    l : numpy.ndarray
        A numpy array of shape (1,) containing a single value representing the regularized
        negative log likelihood of the labels y given the data. If L2 regularization is applied,
        the returned value includes the regularization penalty.

    Notes
    -----
    The regularized negative log likelihood is given by:

        L_reg = - (log_likelihood_positive(h, y) + log_likelihood_negative(h, y))
                + (λ/2) * sum(θ^2)

    where the sum is over all features in θ.

    The bias term (0th dimension of the array is not regularized)
    """
    eps = 1e-8
    h = np.asarray(h).reshape(-1)
    y = np.asarray(y).reshape(-1)
    n = y.size

    # unregularized NLL
    ll_pos = np.sum(y * np.log(h + eps))
    ll_neg = np.sum((1 - y) * np.log(1.0 - h + eps))
    l = -(ll_pos + ll_neg) / n

    # L2 regularization (skip bias at index 0)
    if theta is not None and lambda_ > 0:
        th = np.asarray(theta).reshape(-1)  # (d,)
        l += (lambda_ / (2.0 * n)) * np.sum(th[1:] ** 2)

    return np.array([l], dtype=float)



def gradient_update(theta, X, y, lambda_=0.0):
    """
    Computes the gradient update for logistic regression, including L2 regularization.

    Parameters
    ----------
    theta : numpy.ndarray
        A numpy array of shape (1, n_features + 1) containing the model parameters.

    X : numpy.ndarray
        A numpy array of shape (n_samples, n_features + 1) containing the input feature matrix.

    y : numpy.ndarray
        A numpy array of shape (n_samples,) containing the true labels for the data.
        The labels are 1 for positive examples and 0 for negative examples.

    lambda_ : float, optional
        The regularization strength (λ). This value is used to scale the L2 regularization
        term in the gradient update. Default is 0.0 (no regularization).

    Returns
    -------
    grad : numpy.ndarray
        A numpy array of shape (1, n_features + 1) containing the gradient of the log likelihood
        with respect to the model parameters, including the L2 regularization term if applied.
    """
    X = np.asarray(X, dtype=np.float64)                      # (n, d)
    y = np.asarray(y, dtype=np.float64).reshape(-1, 1)       # (n, 1)
    theta_arr = np.asarray(theta, dtype=np.float64)          # keep original shape
    orig_shape = theta_arr.shape

    # Work in column form internally
    th = theta_arr.reshape(-1, 1)                            # (d, 1)
    n = y.shape[0]

    # Predictions
    h = predict_proba(th, X)                                 # (n, 1)

    # Core gradient
    error = h - y                                            # (n, 1)
    grad_col = (X.T @ error) / n                             # (d, 1)

    # L2 on non-bias (skip index 0)
    if lambda_ != 0:
        reg = (lambda_ / n) * th
        reg[0, 0] = 0.0
        grad_col += reg

    # Return in the same shape as theta
    if len(orig_shape) == 2 and orig_shape[0] == 1:
        return grad_col.T                                    # (1, d)
    elif len(orig_shape) == 1:
        return grad_col.ravel()                              # (d,)
    else:
        return grad_col                                      # (d, 1)


def gradient_descent(theta, X, y, alpha, lambda_, max_iterations, print_iterations):
    """
    Implements the batch gradient descent algorithm for logistic regression,
    with visualization of the decision boundary during the learning process.

    Parameters
    ----------
    theta : numpy.ndarray
        A numpy array of shape (1, n_features + 1) containing the initial model parameters.
        The additional feature is for the bias term.

    X : numpy.ndarray
        A numpy array of shape (n_samples, n_features + 1) containing the input feature matrix.
        Each row corresponds to a sample, and each column corresponds to a feature.

    y : numpy.ndarray
        A numpy array of shape (n_samples,) containing the true labels for the data.
        The labels are 1 for positive examples and 0 for negative examples.

    alpha : float
        The learning rate used to control the step size during the gradient descent update.

    lambda_ : float
        The regularization parameter (λ) used to control the L2 regularization term.

    max_iterations : int
        The maximum number of iterations to run the gradient descent algorithm.

    print_iterations : int
        The number of iterations between each visualization and printing of the loss.
        This allows for observing the learning process over time.

    Returns
    -------
    theta : numpy.ndarray
        A numpy array of shape (1, n_features + 1) containing the optimized model parameters
        after gradient descent has converged or reached the maximum number of iterations.
    """
    iteration = 0

    # --- Add bias column (make X: [1, x1, x2]) ---
    X = np.asarray(X, dtype=np.float64)
    if X.shape[1] == 2:                        # if bias not yet present
        X = np.c_[np.ones((X.shape[0], 1)), X] # (n, 3)

    y = np.asarray(y, dtype=np.float64).reshape(-1, 1)
    theta = np.asarray(theta, dtype=np.float64)  # keep original shape for updates


    while(iteration < max_iterations):
        iteration += 1

        ### YOUR CODE HERE: simultaneous update of partial gradients
        grad = gradient_update(theta, X, y, lambda_)   # same shape as theta
        theta = theta - alpha * grad

        # For every print_iterations number of iterations
        if iteration % print_iterations == 0 or iteration == 1:
            loss = 0

            ### YOUR CODE HERE: calculate the discriminative log likelihood
            ### IMPORTANT: The negative discriminative log likelihood is
            ###             guaranteed to decrease after every iteration
            ###             of the gradient descent algorithm.
            h = predict_proba(theta, X)  # shape (n,1)
            loss = discriminative_negative_log_likelihood(
                h, y, theta=theta.reshape(-1), lambda_=lambda_
            )

            ### END YOUR CODE

            print ("[ Iteration", iteration, "]", "loss =", loss)
            plt.rcParams['figure.figsize'] = (5, 4)
            plt.xlim([20,110])
            plt.ylim([20,110])

            pos = np.where(y == 1)
            neg = np.where(y == 0)

            plt.scatter(X[pos, 1], X[pos, 2], marker='o', c='b')
            plt.scatter(X[neg, 1], X[neg, 2], marker='x', c='r')
            plt.xlabel('Exam 1 score')
            plt.ylabel('Exam 2 score')
            plt.legend(['Admitted', 'Not Admitted'])
            t = np.arange(10, 100, 0.1)

            ### YOUR CODE HERE: plot the decision boundary.
            ### Use t as x-axis values and utilize theta to compute the equation of the line.
            ### Thereafter, use plt.plot to plot the values.
            t = np.arange(10, 100, 0.1)
            th = theta.reshape(-1)  # [θ0, θ1, θ2]
            if th.size >= 3 and th[2] != 0:
                x2_line = -(th[0] + th[1] * t) / th[2]
                plt.plot(t, x2_line)
            plt.show()

    return theta


# Define your hyperparameters here

def get_hyperparameters():
  alpha_test = 0.1
  max_iter = 100000
  print_iter = 10000 # you can experiment with this, but there is no need to change this value for final submission. This won't impact your accuracy.
  lambda_ = 0.0
  return alpha_test, max_iter, print_iter, lambda_


def predict(theta, X):
    """
    Predicts whether the label is 0 or 1 using learned logistic regression parameters.

    Parameters
    ----------
    theta : numpy.ndarray
        A numpy array of shape (1, n_features + 1) containing the learned model parameters,
        including the bias term.

    X : numpy.ndarray
        A numpy array of shape (n_samples, n_features + 1) containing the input feature matrix.
        Each row corresponds to a sample, and each column corresponds to a feature.

    Returns
    -------
    probabilities : numpy.ndarray
        A numpy array of shape (n_samples,) containing the predicted probabilities that each
        sample belongs to the positive class (label 1).

    predicted_labels : numpy.ndarray
        A numpy array of shape (n_samples,) containing the predicted labels (0 or 1) for each
        sample based on the threshold of 0.5.
    """
    probabilities = None
    predicted_labels = None
    # Ensure X has bias column
    X = np.asarray(X, dtype=np.float64)
    if X.shape[1] == theta.shape[1] - 1:   # if missing bias column
        X = np.c_[np.ones((X.shape[0], 1)), X]

    # Compute probabilities
    probabilities = predict_proba(theta, X).reshape(-1)

    # Classify with threshold 0.5
    predicted_labels = (probabilities >= 0.5)

    # Convert boolean array to integer array (0 or 1)
    return probabilities, predicted_labels.astype(int)
